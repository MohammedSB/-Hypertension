{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Utils import *\n",
    "from Utils.Blacksmith import * \n",
    "\n",
    "from Utils.HyMNet import HyMNet\n",
    "from timm.models.layers import trunc_normal_\n",
    "import Utils.ViT as ViT \n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/baharoon/HTN/data/\"\n",
    "CSV_PATH = {\"HTNPath\": PATH + r\"HTN\", \"NonHTNPath\": PATH + \"NonHTN\"}\n",
    "\n",
    "MODELS_PATH = \"/home/baharoon/HTN/Models\"\n",
    "\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baharoon/HTN/HyMNet/Utils/Utils.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['HTN_DM'] = df['HTN'].astype(str) + df['DM'].astype(str)\n",
      "/home/baharoon/HTN/HyMNet/Utils/Utils.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['HTN_DM'] = df['HTN'].astype(str) + df['DM'].astype(str)\n",
      "/home/baharoon/HTN/HyMNet/Utils/Utils.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['HTN_DM'] = df['HTN'].astype(str) + df['DM'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "image_size = 586\n",
    "crop_size = 512\n",
    "\n",
    "# image_size = 256\n",
    "# crop_size = 224\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((image_size, image_size)),\n",
    "    T.CenterCrop(crop_size),\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "    T.RandomRotation(degrees=(0, 360)),\n",
    "    T.GaussianBlur(3),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((image_size, image_size)),\n",
    "    T.CenterCrop(crop_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = HypertensionDataset(CSV_PATH, split=\"train\", train_transform=train_transform)\n",
    "# val_dataset = HypertensionDataset(CSV_PATH, split=\"val\", test_transform=test_transform)\n",
    "val_dataset = HypertensionDataset(CSV_PATH, split=\"val\", test_transform=train_transform)\n",
    "test_dataset = HypertensionDataset(CSV_PATH, split=\"test\", test_transform=test_transform)\n",
    "\n",
    "# train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "epoch_length = math.ceil(len(train_dataset) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(N, trues, probs, threshold=0.5):\n",
    "    \n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "    pr_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    specificity_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for i in range(N):\n",
    "        fpr, tpr, _ = roc_curve(trues[i], probs[i])\n",
    "        precision, recall, _ = precision_recall_curve(trues[i], probs[i])\n",
    "\n",
    "        f1_scores.append(f1_score(trues[i], np.array(probs[i])>=threshold))\n",
    "        auc_scores.append(auc(fpr, tpr))\n",
    "        pr_scores.append(auc(recall, precision))\n",
    "        precision_scores.append(precision_score(trues[i], np.array(probs[i])>=threshold))\n",
    "        recall_scores.append(recall_score(trues[i], np.array(probs[i])>=threshold))\n",
    "        specificity_scores.append(recall_score(trues[i], np.array(probs[i])>=threshold, pos_label=0))\n",
    "        accuracy_scores.append(accuracy_score(trues[i], np.array(probs[i])>=threshold))\n",
    "        \n",
    "    results = {\"f1\": {}, \"auroc\": {}, \"auprc\": {}, \"precision\": {},\n",
    "               \"recall\": {}, \"specificity\": {}, \"accuracy\": {}}\n",
    "    \n",
    "    results[\"f1\"][\"range\"] = [round(s, 3) for s in np.percentile(f1_scores, [2.5, 97.5])]  \n",
    "    results[\"f1\"][\"avrg\"] = sum(results[\"f1\"][\"range\"]) / 2\n",
    "    \n",
    "    results[\"auroc\"][\"range\"] = [round(s, 3) for s in np.percentile(auc_scores, [2.5, 97.5])]  \n",
    "    results[\"auroc\"][\"avrg\"] = sum(results[\"auroc\"][\"range\"]) / 2\n",
    "    \n",
    "    results[\"auprc\"][\"range\"] = [round(s, 3) for s in np.percentile(pr_scores, [2.5, 97.5])]  \n",
    "    results[\"auprc\"][\"avrg\"] = sum(results[\"auprc\"][\"range\"]) / 2\n",
    "    \n",
    "    results[\"precision\"][\"range\"] = [round(s, 3) for s in np.percentile(precision_scores, [2.5, 97.5])]  \n",
    "    results[\"precision\"][\"avrg\"] = sum(results[\"precision\"][\"range\"]) / 2\n",
    "    \n",
    "    results[\"recall\"][\"range\"] = [round(s, 3) for s in np.percentile(recall_scores, [2.5, 97.5])]  \n",
    "    results[\"recall\"][\"avrg\"] = sum(results[\"recall\"][\"range\"]) / 2\n",
    "    \n",
    "    results[\"specificity\"][\"range\"] = [round(s, 3) for s in np.percentile(specificity_scores, [2.5, 97.5])]  \n",
    "    results[\"specificity\"][\"avrg\"] = sum(results[\"specificity\"][\"range\"]) / 2\n",
    "    \n",
    "    results[\"accuracy\"][\"range\"] = [round(s, 3) for s in np.percentile(accuracy_scores, [2.5, 97.5])]  \n",
    "    results[\"accuracy\"][\"avrg\"] = sum(results[\"accuracy\"][\"range\"]) / 2\n",
    "    \n",
    "    print(f\"f1: {results['f1']['range']}\",\n",
    "            f\"average: {round(results['f1']['avrg'], 3)}\") \n",
    "    print(f\"auroc: {results['auroc']['range']}\",\n",
    "            f\"average: {round(results['auroc']['avrg'], 3)}\") \n",
    "    print(f\"auprc: {results['auprc']['range']}\",\n",
    "            f\"average: {round(results['auprc']['avrg'], 3)}\") \n",
    "    print(f\"precision: {results['precision']['range']}\",\n",
    "            f\"average: {round(results['precision']['avrg'], 3)}\") \n",
    "    print(f\"recall: {results['recall']['range']}\",\n",
    "            f\"average: {round(results['recall']['avrg'], 3)}\") \n",
    "    print(f\"specificity: {results['specificity']['range']}\",\n",
    "            f\"average: {round(results['specificity']['avrg'], 3)}\") \n",
    "    print(f\"accuracy: {results['accuracy']['range']}\",\n",
    "            f\"average: {round(results['accuracy']['avrg'], 3)}\") \n",
    "    \n",
    "    return results\n",
    "    \n",
    "    \n",
    "def boostrap(N, model, data_loader, save_loc=None):\n",
    "    from tqdm import tqdm\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        if hasattr(data_loader.dataset, 'df'):\n",
    "            if \"image_prob\" in data_loader.dataset.df.columns:\n",
    "                probas = model.predict_proba(data_loader.dataset.df[[\"image_logit\",\"Age\", \"Gender\"]])\n",
    "            else:\n",
    "                probas = model.predict_proba(data_loader.dataset.df[[\"Age\", \"Gender\"]])\n",
    "        else:        \n",
    "            probas = model.predict_proba(data_loader.dataset.x)\n",
    "        preds = probas[:, 1].tolist()\n",
    "        \n",
    "        if hasattr(data_loader.dataset, 'df'): \n",
    "            trues = data_loader.dataset.df[\"HTN\"].tolist()\n",
    "        else:\n",
    "            trues = data_loader.dataset.y.tolist()\n",
    "    else:\n",
    "        for sample in tqdm(data_loader):\n",
    "            image, features, target = sample[\"image\"].to(device).float(), sample[\"features\"].to(device).float(), sample[\"label\"]\n",
    "            output = model(image, features)\n",
    "            preds.append(torch.nn.Sigmoid()(output.squeeze().cpu().detach()).item())\n",
    "            trues.append(target.cpu().detach().item())\n",
    "            \n",
    "    if save_loc != None:\n",
    "        results = dict()\n",
    "        results[\"true\"] = trues\n",
    "        results[\"prob\"] = preds\n",
    "        save_loc_preds = \"\".join(save_loc.split(\".\")[:-1]) + \"outputs.json\"\n",
    "        with open(save_loc_preds, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    n_samples = len(data_loader)\n",
    "    indices = [i for i in range(n_samples)]\n",
    "\n",
    "    bootstrap_preds, bootstrap_trues = [], []\n",
    "    for run in range(N):\n",
    "        bootstrapped_indices = resample(indices, replace=True, n_samples=n_samples)\n",
    "        preds_tmp , trues_tmp = [preds[indx] for indx in bootstrapped_indices], [trues[indx] for indx in bootstrapped_indices]\n",
    "        bootstrap_preds.append(preds_tmp)\n",
    "        bootstrap_trues.append(trues_tmp)\n",
    "        \n",
    "    if save_loc != None:\n",
    "        results = dict()\n",
    "        results[\"true\"] = bootstrap_trues\n",
    "        results[\"prob\"] = bootstrap_preds\n",
    "        save_loc_preds = \"\".join(save_loc.split(\".\")[:-1]) + \"bootstrap.json\"\n",
    "        with open(save_loc_preds, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "    results = calculate_results(N, bootstrap_trues, bootstrap_preds)\n",
    "    \n",
    "    if save_loc != None:\n",
    "        with open(save_loc, 'w') as f:\n",
    "            # Use json.dump to write the dictionary to the file\n",
    "            json.dump(results, f)\n",
    "            \n",
    "def binary_bootstrap(model1, model2, data_loader, device=None):\n",
    "    preds1, preds2, trues = [], [], []\n",
    "    \n",
    "    for sample in tqdm(data_loader):\n",
    "        image, features, target = sample[\"image\"].to(device).float(), sample[\"features\"].to(device).float(), sample[\"label\"]\n",
    "        output1 = model1(image, features)\n",
    "        preds1.append(torch.nn.Sigmoid()(output1.squeeze().cpu().detach()).item())\n",
    "        \n",
    "        output2 = model2(image, features)\n",
    "        preds2.append(torch.nn.Sigmoid()(output2.squeeze().cpu().detach()).item())\n",
    "        \n",
    "        trues.append(target.cpu().detach().item())\n",
    "        \n",
    "    n_samples = len(data_loader)\n",
    "    indices = [i for i in range(n_samples)]\n",
    "\n",
    "    bootstrap_preds1, bootstrap_preds2, bootstrap_trues = [], [], []\n",
    "    for run in range(N):\n",
    "        bootstrapped_indices = resample(indices, replace=True, n_samples=n_samples)\n",
    "        preds1_tmp, preds2_tmp, trues_tmp =  [preds1[indx] for indx in bootstrapped_indices],\\\n",
    "                                            [preds2[indx] for indx in bootstrapped_indices], \\\n",
    "                                            [trues[indx] for indx in bootstrapped_indices]\n",
    "        \n",
    "        bootstrap_preds1.append(preds1_tmp)\n",
    "        bootstrap_preds2.append(preds2_tmp)\n",
    "        bootstrap_trues.append(trues_tmp)\n",
    "    \n",
    "    results = {}\n",
    "    results[\"true\"] = bootstrap_trues\n",
    "    results[\"prob1\"] = bootstrap_preds1\n",
    "    results[\"prob2\"] = bootstrap_preds2\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "\n",
    "FM_PATH = MODELS_PATH + r\"/FundusModel.pth\"\n",
    "DM_PATH = MODELS_PATH + r\"/DemographicFCNN.pth\"\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FundusModel Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(FM_PATH)\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=224)\n",
    "\n",
    "model = HyMNet(image_model=image_model).to(device)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch in Progress: 100%|████████████████████████████████████████| 1007/1007 [00:59<00:00, 17.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Average Loss:  0.614882,    Accuracy: 65.44%,    Correct Counter: 659/1007,    F1 Score: 0.73,    Precision: 0.68,    Recall:  0.79,    PR:  0.80,    AUROC:  0.70\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = test(model, criterion, test_loader, device, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1007/1007 [01:34<00:00, 10.67it/s]\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=model, data_loader=test_loader, save_loc=f\"{os.getcwd() + os.sep}Results/FundusModel.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DemographicModels Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train_dataset.df[[\"Age\", \"Gender\"]], train_dataset.df[\"HTN\"]\n",
    "val_x, val_y = val_dataset.df[[\"Age\", \"Gender\"]], val_dataset.df[\"HTN\"]\n",
    "train_x, train_y = pd.concat([train_x, val_x]), pd.concat([train_y, val_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODELS_PATH+'/DemographicXGBParams.json') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "boost = xgb.XGBClassifier(**params, objective=\"binary:logistic\")\n",
    "boost.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boostrap(N, boost, test_loader, save_loc=f\"{os.getcwd() + os.sep}Results/DemographicXGB.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "with open(MODELS_PATH+'/DemographicSVMParams.json') as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "svm = SVC(**params, probability=True)\n",
    "\n",
    "svm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boostrap(N, svm, test_loader, save_loc=f\"{os.getcwd() + os.sep}Results/DemographicSVM.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ").to(device)\n",
    "\n",
    "model = HyMNet(tabular_model=tabular_model) \n",
    "model.load_state_dict(torch.load(MODELS_PATH + \"/DemographicFCNN.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1007/1007 [00:54<00:00, 18.41it/s]\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=model, data_loader=test_loader, save_loc=f\"{os.getcwd() + os.sep}Results/DemographicFCNN.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FusionModel Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JointFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 14x14 to 32x32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    ")\n",
    "\n",
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=8).requires_grad_(True)\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=40, out_features=128),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=128, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")\n",
    "\n",
    "model = HyMNet(image_model=image_model, tabular_model=tabular_model, fusion_model=fusion_model).to(device)\n",
    "state_dict = torch.load(MODELS_PATH + \"/JointFusion_finetune.pth\")\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1007/1007 [01:34<00:00, 10.67it/s]\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=model, data_loader=test_loader, save_loc=f\"{os.getcwd() + os.sep}Results/JointFusion.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PredictionFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 14x14 to 32x32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=1).requires_grad_(True)\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")\n",
    "\n",
    "model = HyMNet(image_model=image_model, tabular_model=tabular_model, fusion_model=fusion_model).to(device)\n",
    "state_dict = torch.load(MODELS_PATH + \"/PredFusion.pth\")\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1007/1007 [01:39<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: [0.733, 0.784] average: 0.758\n",
      "auroc: [0.637, 0.707] average: 0.672\n",
      "auprc: [0.671, 0.755] average: 0.713\n",
      "precision: [0.646, 0.713] average: 0.68\n",
      "recall: [0.833, 0.888] average: 0.86\n",
      "specificity: [0.364, 0.46] average: 0.412\n",
      "accuracy: [0.647, 0.706] average: 0.676\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=model, data_loader=test_loader, save_loc=f\"{os.getcwd() + os.sep}Results/PredFusion.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LateFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 14x14 to 32x32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=1).requires_grad_(True)\n",
    "model = HyMNet(image_model=image_model).to(device)\n",
    "state_dict = torch.load(FM_PATH)\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_tabular_dataset(model, train_dataset, device, method=\"lf\")\n",
    "test_x, test_y = build_tabular_dataset(model, test_dataset, device, method=\"lf\")\n",
    "\n",
    "train_late_fusion = InputOutputDataset(train_x, train_y)\n",
    "test_late_fusion = InputOutputDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_late_fusion, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_late_fusion, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.001, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODELS_PATH+ '/LateFusionXGBParams.json') as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "boost = xgb.XGBClassifier(**params, objective=\"binary:logistic\")\n",
    "\n",
    "boost.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: [0.733, 0.783] average: 0.758\n",
      "auroc: [0.684, 0.75] average: 0.717\n",
      "auprc: [0.708, 0.791] average: 0.75\n",
      "precision: [0.652, 0.719] average: 0.686\n",
      "recall: [0.819, 0.877] average: 0.848\n",
      "specificity: [0.388, 0.484] average: 0.436\n",
      "accuracy: [0.652, 0.708] average: 0.68\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=boost, data_loader=test_loader, save_loc=f\"{os.getcwd() + os.sep}Results/LateFusionXGB.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, gamma=0.01, kernel='poly', probability=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "with open(MODELS_PATH+ '/LateFusionSVMParams.json') as f:\n",
    "    params = json.load(f)\n",
    "        \n",
    "svm = SVC(**params, probability=True)\n",
    "\n",
    "svm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: [0.744, 0.792] average: 0.768\n",
      "auroc: [0.652, 0.718] average: 0.685\n",
      "auprc: [0.692, 0.773] average: 0.732\n",
      "precision: [0.625, 0.689] average: 0.657\n",
      "recall: [0.905, 0.946] average: 0.926\n",
      "specificity: [0.256, 0.343] average: 0.3\n",
      "accuracy: [0.641, 0.699] average: 0.67\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=svm, data_loader=test_loader, save_loc=f\"{os.getcwd() + os.sep}Results/LateFusionSVM.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_path = MODELS_PATH + r\"/LateFusionFCNN.pth\"\n",
    "\n",
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "model = HyMNet(tabular_model=tabular_model).to(device)\n",
    "model.load_state_dict(torch.load(fm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1007/1007 [00:00<00:00, 2137.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: [0.726, 0.777] average: 0.752\n",
      "auroc: [0.664, 0.73] average: 0.697\n",
      "auprc: [0.703, 0.782] average: 0.742\n",
      "precision: [0.649, 0.715] average: 0.682\n",
      "recall: [0.807, 0.866] average: 0.836\n",
      "specificity: [0.388, 0.482] average: 0.435\n",
      "accuracy: [0.644, 0.702] average: 0.673\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=model, data_loader=test_loader, save_loc=f\"{os.getcwd() + os.sep}Results/LateFusionFCNN.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 14x14 to 32x32\n",
      "Position interpolate from 14x14 to 32x32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['image_model.cls_token', 'image_model.pos_embed', 'image_model.patch_embed.proj.weight', 'image_model.patch_embed.proj.bias', 'image_model.blocks.0.norm1.weight', 'image_model.blocks.0.norm1.bias', 'image_model.blocks.0.attn.qkv.weight', 'image_model.blocks.0.attn.qkv.bias', 'image_model.blocks.0.attn.proj.weight', 'image_model.blocks.0.attn.proj.bias', 'image_model.blocks.0.norm2.weight', 'image_model.blocks.0.norm2.bias', 'image_model.blocks.0.mlp.fc1.weight', 'image_model.blocks.0.mlp.fc1.bias', 'image_model.blocks.0.mlp.fc2.weight', 'image_model.blocks.0.mlp.fc2.bias', 'image_model.blocks.1.norm1.weight', 'image_model.blocks.1.norm1.bias', 'image_model.blocks.1.attn.qkv.weight', 'image_model.blocks.1.attn.qkv.bias', 'image_model.blocks.1.attn.proj.weight', 'image_model.blocks.1.attn.proj.bias', 'image_model.blocks.1.norm2.weight', 'image_model.blocks.1.norm2.bias', 'image_model.blocks.1.mlp.fc1.weight', 'image_model.blocks.1.mlp.fc1.bias', 'image_model.blocks.1.mlp.fc2.weight', 'image_model.blocks.1.mlp.fc2.bias', 'image_model.blocks.2.norm1.weight', 'image_model.blocks.2.norm1.bias', 'image_model.blocks.2.attn.qkv.weight', 'image_model.blocks.2.attn.qkv.bias', 'image_model.blocks.2.attn.proj.weight', 'image_model.blocks.2.attn.proj.bias', 'image_model.blocks.2.norm2.weight', 'image_model.blocks.2.norm2.bias', 'image_model.blocks.2.mlp.fc1.weight', 'image_model.blocks.2.mlp.fc1.bias', 'image_model.blocks.2.mlp.fc2.weight', 'image_model.blocks.2.mlp.fc2.bias', 'image_model.blocks.3.norm1.weight', 'image_model.blocks.3.norm1.bias', 'image_model.blocks.3.attn.qkv.weight', 'image_model.blocks.3.attn.qkv.bias', 'image_model.blocks.3.attn.proj.weight', 'image_model.blocks.3.attn.proj.bias', 'image_model.blocks.3.norm2.weight', 'image_model.blocks.3.norm2.bias', 'image_model.blocks.3.mlp.fc1.weight', 'image_model.blocks.3.mlp.fc1.bias', 'image_model.blocks.3.mlp.fc2.weight', 'image_model.blocks.3.mlp.fc2.bias', 'image_model.blocks.4.norm1.weight', 'image_model.blocks.4.norm1.bias', 'image_model.blocks.4.attn.qkv.weight', 'image_model.blocks.4.attn.qkv.bias', 'image_model.blocks.4.attn.proj.weight', 'image_model.blocks.4.attn.proj.bias', 'image_model.blocks.4.norm2.weight', 'image_model.blocks.4.norm2.bias', 'image_model.blocks.4.mlp.fc1.weight', 'image_model.blocks.4.mlp.fc1.bias', 'image_model.blocks.4.mlp.fc2.weight', 'image_model.blocks.4.mlp.fc2.bias', 'image_model.blocks.5.norm1.weight', 'image_model.blocks.5.norm1.bias', 'image_model.blocks.5.attn.qkv.weight', 'image_model.blocks.5.attn.qkv.bias', 'image_model.blocks.5.attn.proj.weight', 'image_model.blocks.5.attn.proj.bias', 'image_model.blocks.5.norm2.weight', 'image_model.blocks.5.norm2.bias', 'image_model.blocks.5.mlp.fc1.weight', 'image_model.blocks.5.mlp.fc1.bias', 'image_model.blocks.5.mlp.fc2.weight', 'image_model.blocks.5.mlp.fc2.bias', 'image_model.blocks.6.norm1.weight', 'image_model.blocks.6.norm1.bias', 'image_model.blocks.6.attn.qkv.weight', 'image_model.blocks.6.attn.qkv.bias', 'image_model.blocks.6.attn.proj.weight', 'image_model.blocks.6.attn.proj.bias', 'image_model.blocks.6.norm2.weight', 'image_model.blocks.6.norm2.bias', 'image_model.blocks.6.mlp.fc1.weight', 'image_model.blocks.6.mlp.fc1.bias', 'image_model.blocks.6.mlp.fc2.weight', 'image_model.blocks.6.mlp.fc2.bias', 'image_model.blocks.7.norm1.weight', 'image_model.blocks.7.norm1.bias', 'image_model.blocks.7.attn.qkv.weight', 'image_model.blocks.7.attn.qkv.bias', 'image_model.blocks.7.attn.proj.weight', 'image_model.blocks.7.attn.proj.bias', 'image_model.blocks.7.norm2.weight', 'image_model.blocks.7.norm2.bias', 'image_model.blocks.7.mlp.fc1.weight', 'image_model.blocks.7.mlp.fc1.bias', 'image_model.blocks.7.mlp.fc2.weight', 'image_model.blocks.7.mlp.fc2.bias', 'image_model.blocks.8.norm1.weight', 'image_model.blocks.8.norm1.bias', 'image_model.blocks.8.attn.qkv.weight', 'image_model.blocks.8.attn.qkv.bias', 'image_model.blocks.8.attn.proj.weight', 'image_model.blocks.8.attn.proj.bias', 'image_model.blocks.8.norm2.weight', 'image_model.blocks.8.norm2.bias', 'image_model.blocks.8.mlp.fc1.weight', 'image_model.blocks.8.mlp.fc1.bias', 'image_model.blocks.8.mlp.fc2.weight', 'image_model.blocks.8.mlp.fc2.bias', 'image_model.blocks.9.norm1.weight', 'image_model.blocks.9.norm1.bias', 'image_model.blocks.9.attn.qkv.weight', 'image_model.blocks.9.attn.qkv.bias', 'image_model.blocks.9.attn.proj.weight', 'image_model.blocks.9.attn.proj.bias', 'image_model.blocks.9.norm2.weight', 'image_model.blocks.9.norm2.bias', 'image_model.blocks.9.mlp.fc1.weight', 'image_model.blocks.9.mlp.fc1.bias', 'image_model.blocks.9.mlp.fc2.weight', 'image_model.blocks.9.mlp.fc2.bias', 'image_model.blocks.10.norm1.weight', 'image_model.blocks.10.norm1.bias', 'image_model.blocks.10.attn.qkv.weight', 'image_model.blocks.10.attn.qkv.bias', 'image_model.blocks.10.attn.proj.weight', 'image_model.blocks.10.attn.proj.bias', 'image_model.blocks.10.norm2.weight', 'image_model.blocks.10.norm2.bias', 'image_model.blocks.10.mlp.fc1.weight', 'image_model.blocks.10.mlp.fc1.bias', 'image_model.blocks.10.mlp.fc2.weight', 'image_model.blocks.10.mlp.fc2.bias', 'image_model.blocks.11.norm1.weight', 'image_model.blocks.11.norm1.bias', 'image_model.blocks.11.attn.qkv.weight', 'image_model.blocks.11.attn.qkv.bias', 'image_model.blocks.11.attn.proj.weight', 'image_model.blocks.11.attn.proj.bias', 'image_model.blocks.11.norm2.weight', 'image_model.blocks.11.norm2.bias', 'image_model.blocks.11.mlp.fc1.weight', 'image_model.blocks.11.mlp.fc1.bias', 'image_model.blocks.11.mlp.fc2.weight', 'image_model.blocks.11.mlp.fc2.bias', 'image_model.blocks.12.norm1.weight', 'image_model.blocks.12.norm1.bias', 'image_model.blocks.12.attn.qkv.weight', 'image_model.blocks.12.attn.qkv.bias', 'image_model.blocks.12.attn.proj.weight', 'image_model.blocks.12.attn.proj.bias', 'image_model.blocks.12.norm2.weight', 'image_model.blocks.12.norm2.bias', 'image_model.blocks.12.mlp.fc1.weight', 'image_model.blocks.12.mlp.fc1.bias', 'image_model.blocks.12.mlp.fc2.weight', 'image_model.blocks.12.mlp.fc2.bias', 'image_model.blocks.13.norm1.weight', 'image_model.blocks.13.norm1.bias', 'image_model.blocks.13.attn.qkv.weight', 'image_model.blocks.13.attn.qkv.bias', 'image_model.blocks.13.attn.proj.weight', 'image_model.blocks.13.attn.proj.bias', 'image_model.blocks.13.norm2.weight', 'image_model.blocks.13.norm2.bias', 'image_model.blocks.13.mlp.fc1.weight', 'image_model.blocks.13.mlp.fc1.bias', 'image_model.blocks.13.mlp.fc2.weight', 'image_model.blocks.13.mlp.fc2.bias', 'image_model.blocks.14.norm1.weight', 'image_model.blocks.14.norm1.bias', 'image_model.blocks.14.attn.qkv.weight', 'image_model.blocks.14.attn.qkv.bias', 'image_model.blocks.14.attn.proj.weight', 'image_model.blocks.14.attn.proj.bias', 'image_model.blocks.14.norm2.weight', 'image_model.blocks.14.norm2.bias', 'image_model.blocks.14.mlp.fc1.weight', 'image_model.blocks.14.mlp.fc1.bias', 'image_model.blocks.14.mlp.fc2.weight', 'image_model.blocks.14.mlp.fc2.bias', 'image_model.blocks.15.norm1.weight', 'image_model.blocks.15.norm1.bias', 'image_model.blocks.15.attn.qkv.weight', 'image_model.blocks.15.attn.qkv.bias', 'image_model.blocks.15.attn.proj.weight', 'image_model.blocks.15.attn.proj.bias', 'image_model.blocks.15.norm2.weight', 'image_model.blocks.15.norm2.bias', 'image_model.blocks.15.mlp.fc1.weight', 'image_model.blocks.15.mlp.fc1.bias', 'image_model.blocks.15.mlp.fc2.weight', 'image_model.blocks.15.mlp.fc2.bias', 'image_model.blocks.16.norm1.weight', 'image_model.blocks.16.norm1.bias', 'image_model.blocks.16.attn.qkv.weight', 'image_model.blocks.16.attn.qkv.bias', 'image_model.blocks.16.attn.proj.weight', 'image_model.blocks.16.attn.proj.bias', 'image_model.blocks.16.norm2.weight', 'image_model.blocks.16.norm2.bias', 'image_model.blocks.16.mlp.fc1.weight', 'image_model.blocks.16.mlp.fc1.bias', 'image_model.blocks.16.mlp.fc2.weight', 'image_model.blocks.16.mlp.fc2.bias', 'image_model.blocks.17.norm1.weight', 'image_model.blocks.17.norm1.bias', 'image_model.blocks.17.attn.qkv.weight', 'image_model.blocks.17.attn.qkv.bias', 'image_model.blocks.17.attn.proj.weight', 'image_model.blocks.17.attn.proj.bias', 'image_model.blocks.17.norm2.weight', 'image_model.blocks.17.norm2.bias', 'image_model.blocks.17.mlp.fc1.weight', 'image_model.blocks.17.mlp.fc1.bias', 'image_model.blocks.17.mlp.fc2.weight', 'image_model.blocks.17.mlp.fc2.bias', 'image_model.blocks.18.norm1.weight', 'image_model.blocks.18.norm1.bias', 'image_model.blocks.18.attn.qkv.weight', 'image_model.blocks.18.attn.qkv.bias', 'image_model.blocks.18.attn.proj.weight', 'image_model.blocks.18.attn.proj.bias', 'image_model.blocks.18.norm2.weight', 'image_model.blocks.18.norm2.bias', 'image_model.blocks.18.mlp.fc1.weight', 'image_model.blocks.18.mlp.fc1.bias', 'image_model.blocks.18.mlp.fc2.weight', 'image_model.blocks.18.mlp.fc2.bias', 'image_model.blocks.19.norm1.weight', 'image_model.blocks.19.norm1.bias', 'image_model.blocks.19.attn.qkv.weight', 'image_model.blocks.19.attn.qkv.bias', 'image_model.blocks.19.attn.proj.weight', 'image_model.blocks.19.attn.proj.bias', 'image_model.blocks.19.norm2.weight', 'image_model.blocks.19.norm2.bias', 'image_model.blocks.19.mlp.fc1.weight', 'image_model.blocks.19.mlp.fc1.bias', 'image_model.blocks.19.mlp.fc2.weight', 'image_model.blocks.19.mlp.fc2.bias', 'image_model.blocks.20.norm1.weight', 'image_model.blocks.20.norm1.bias', 'image_model.blocks.20.attn.qkv.weight', 'image_model.blocks.20.attn.qkv.bias', 'image_model.blocks.20.attn.proj.weight', 'image_model.blocks.20.attn.proj.bias', 'image_model.blocks.20.norm2.weight', 'image_model.blocks.20.norm2.bias', 'image_model.blocks.20.mlp.fc1.weight', 'image_model.blocks.20.mlp.fc1.bias', 'image_model.blocks.20.mlp.fc2.weight', 'image_model.blocks.20.mlp.fc2.bias', 'image_model.blocks.21.norm1.weight', 'image_model.blocks.21.norm1.bias', 'image_model.blocks.21.attn.qkv.weight', 'image_model.blocks.21.attn.qkv.bias', 'image_model.blocks.21.attn.proj.weight', 'image_model.blocks.21.attn.proj.bias', 'image_model.blocks.21.norm2.weight', 'image_model.blocks.21.norm2.bias', 'image_model.blocks.21.mlp.fc1.weight', 'image_model.blocks.21.mlp.fc1.bias', 'image_model.blocks.21.mlp.fc2.weight', 'image_model.blocks.21.mlp.fc2.bias', 'image_model.blocks.22.norm1.weight', 'image_model.blocks.22.norm1.bias', 'image_model.blocks.22.attn.qkv.weight', 'image_model.blocks.22.attn.qkv.bias', 'image_model.blocks.22.attn.proj.weight', 'image_model.blocks.22.attn.proj.bias', 'image_model.blocks.22.norm2.weight', 'image_model.blocks.22.norm2.bias', 'image_model.blocks.22.mlp.fc1.weight', 'image_model.blocks.22.mlp.fc1.bias', 'image_model.blocks.22.mlp.fc2.weight', 'image_model.blocks.22.mlp.fc2.bias', 'image_model.blocks.23.norm1.weight', 'image_model.blocks.23.norm1.bias', 'image_model.blocks.23.attn.qkv.weight', 'image_model.blocks.23.attn.qkv.bias', 'image_model.blocks.23.attn.proj.weight', 'image_model.blocks.23.attn.proj.bias', 'image_model.blocks.23.norm2.weight', 'image_model.blocks.23.norm2.bias', 'image_model.blocks.23.mlp.fc1.weight', 'image_model.blocks.23.mlp.fc1.bias', 'image_model.blocks.23.mlp.fc2.weight', 'image_model.blocks.23.mlp.fc2.bias', 'image_model.head.weight', 'image_model.head.bias', 'image_model.fc_norm.weight', 'image_model.fc_norm.bias', 'fusion_model.image_model.cls_token', 'fusion_model.image_model.pos_embed', 'fusion_model.image_model.patch_embed.proj.weight', 'fusion_model.image_model.patch_embed.proj.bias', 'fusion_model.image_model.blocks.0.norm1.weight', 'fusion_model.image_model.blocks.0.norm1.bias', 'fusion_model.image_model.blocks.0.attn.qkv.weight', 'fusion_model.image_model.blocks.0.attn.qkv.bias', 'fusion_model.image_model.blocks.0.attn.proj.weight', 'fusion_model.image_model.blocks.0.attn.proj.bias', 'fusion_model.image_model.blocks.0.norm2.weight', 'fusion_model.image_model.blocks.0.norm2.bias', 'fusion_model.image_model.blocks.0.mlp.fc1.weight', 'fusion_model.image_model.blocks.0.mlp.fc1.bias', 'fusion_model.image_model.blocks.0.mlp.fc2.weight', 'fusion_model.image_model.blocks.0.mlp.fc2.bias', 'fusion_model.image_model.blocks.1.norm1.weight', 'fusion_model.image_model.blocks.1.norm1.bias', 'fusion_model.image_model.blocks.1.attn.qkv.weight', 'fusion_model.image_model.blocks.1.attn.qkv.bias', 'fusion_model.image_model.blocks.1.attn.proj.weight', 'fusion_model.image_model.blocks.1.attn.proj.bias', 'fusion_model.image_model.blocks.1.norm2.weight', 'fusion_model.image_model.blocks.1.norm2.bias', 'fusion_model.image_model.blocks.1.mlp.fc1.weight', 'fusion_model.image_model.blocks.1.mlp.fc1.bias', 'fusion_model.image_model.blocks.1.mlp.fc2.weight', 'fusion_model.image_model.blocks.1.mlp.fc2.bias', 'fusion_model.image_model.blocks.2.norm1.weight', 'fusion_model.image_model.blocks.2.norm1.bias', 'fusion_model.image_model.blocks.2.attn.qkv.weight', 'fusion_model.image_model.blocks.2.attn.qkv.bias', 'fusion_model.image_model.blocks.2.attn.proj.weight', 'fusion_model.image_model.blocks.2.attn.proj.bias', 'fusion_model.image_model.blocks.2.norm2.weight', 'fusion_model.image_model.blocks.2.norm2.bias', 'fusion_model.image_model.blocks.2.mlp.fc1.weight', 'fusion_model.image_model.blocks.2.mlp.fc1.bias', 'fusion_model.image_model.blocks.2.mlp.fc2.weight', 'fusion_model.image_model.blocks.2.mlp.fc2.bias', 'fusion_model.image_model.blocks.3.norm1.weight', 'fusion_model.image_model.blocks.3.norm1.bias', 'fusion_model.image_model.blocks.3.attn.qkv.weight', 'fusion_model.image_model.blocks.3.attn.qkv.bias', 'fusion_model.image_model.blocks.3.attn.proj.weight', 'fusion_model.image_model.blocks.3.attn.proj.bias', 'fusion_model.image_model.blocks.3.norm2.weight', 'fusion_model.image_model.blocks.3.norm2.bias', 'fusion_model.image_model.blocks.3.mlp.fc1.weight', 'fusion_model.image_model.blocks.3.mlp.fc1.bias', 'fusion_model.image_model.blocks.3.mlp.fc2.weight', 'fusion_model.image_model.blocks.3.mlp.fc2.bias', 'fusion_model.image_model.blocks.4.norm1.weight', 'fusion_model.image_model.blocks.4.norm1.bias', 'fusion_model.image_model.blocks.4.attn.qkv.weight', 'fusion_model.image_model.blocks.4.attn.qkv.bias', 'fusion_model.image_model.blocks.4.attn.proj.weight', 'fusion_model.image_model.blocks.4.attn.proj.bias', 'fusion_model.image_model.blocks.4.norm2.weight', 'fusion_model.image_model.blocks.4.norm2.bias', 'fusion_model.image_model.blocks.4.mlp.fc1.weight', 'fusion_model.image_model.blocks.4.mlp.fc1.bias', 'fusion_model.image_model.blocks.4.mlp.fc2.weight', 'fusion_model.image_model.blocks.4.mlp.fc2.bias', 'fusion_model.image_model.blocks.5.norm1.weight', 'fusion_model.image_model.blocks.5.norm1.bias', 'fusion_model.image_model.blocks.5.attn.qkv.weight', 'fusion_model.image_model.blocks.5.attn.qkv.bias', 'fusion_model.image_model.blocks.5.attn.proj.weight', 'fusion_model.image_model.blocks.5.attn.proj.bias', 'fusion_model.image_model.blocks.5.norm2.weight', 'fusion_model.image_model.blocks.5.norm2.bias', 'fusion_model.image_model.blocks.5.mlp.fc1.weight', 'fusion_model.image_model.blocks.5.mlp.fc1.bias', 'fusion_model.image_model.blocks.5.mlp.fc2.weight', 'fusion_model.image_model.blocks.5.mlp.fc2.bias', 'fusion_model.image_model.blocks.6.norm1.weight', 'fusion_model.image_model.blocks.6.norm1.bias', 'fusion_model.image_model.blocks.6.attn.qkv.weight', 'fusion_model.image_model.blocks.6.attn.qkv.bias', 'fusion_model.image_model.blocks.6.attn.proj.weight', 'fusion_model.image_model.blocks.6.attn.proj.bias', 'fusion_model.image_model.blocks.6.norm2.weight', 'fusion_model.image_model.blocks.6.norm2.bias', 'fusion_model.image_model.blocks.6.mlp.fc1.weight', 'fusion_model.image_model.blocks.6.mlp.fc1.bias', 'fusion_model.image_model.blocks.6.mlp.fc2.weight', 'fusion_model.image_model.blocks.6.mlp.fc2.bias', 'fusion_model.image_model.blocks.7.norm1.weight', 'fusion_model.image_model.blocks.7.norm1.bias', 'fusion_model.image_model.blocks.7.attn.qkv.weight', 'fusion_model.image_model.blocks.7.attn.qkv.bias', 'fusion_model.image_model.blocks.7.attn.proj.weight', 'fusion_model.image_model.blocks.7.attn.proj.bias', 'fusion_model.image_model.blocks.7.norm2.weight', 'fusion_model.image_model.blocks.7.norm2.bias', 'fusion_model.image_model.blocks.7.mlp.fc1.weight', 'fusion_model.image_model.blocks.7.mlp.fc1.bias', 'fusion_model.image_model.blocks.7.mlp.fc2.weight', 'fusion_model.image_model.blocks.7.mlp.fc2.bias', 'fusion_model.image_model.blocks.8.norm1.weight', 'fusion_model.image_model.blocks.8.norm1.bias', 'fusion_model.image_model.blocks.8.attn.qkv.weight', 'fusion_model.image_model.blocks.8.attn.qkv.bias', 'fusion_model.image_model.blocks.8.attn.proj.weight', 'fusion_model.image_model.blocks.8.attn.proj.bias', 'fusion_model.image_model.blocks.8.norm2.weight', 'fusion_model.image_model.blocks.8.norm2.bias', 'fusion_model.image_model.blocks.8.mlp.fc1.weight', 'fusion_model.image_model.blocks.8.mlp.fc1.bias', 'fusion_model.image_model.blocks.8.mlp.fc2.weight', 'fusion_model.image_model.blocks.8.mlp.fc2.bias', 'fusion_model.image_model.blocks.9.norm1.weight', 'fusion_model.image_model.blocks.9.norm1.bias', 'fusion_model.image_model.blocks.9.attn.qkv.weight', 'fusion_model.image_model.blocks.9.attn.qkv.bias', 'fusion_model.image_model.blocks.9.attn.proj.weight', 'fusion_model.image_model.blocks.9.attn.proj.bias', 'fusion_model.image_model.blocks.9.norm2.weight', 'fusion_model.image_model.blocks.9.norm2.bias', 'fusion_model.image_model.blocks.9.mlp.fc1.weight', 'fusion_model.image_model.blocks.9.mlp.fc1.bias', 'fusion_model.image_model.blocks.9.mlp.fc2.weight', 'fusion_model.image_model.blocks.9.mlp.fc2.bias', 'fusion_model.image_model.blocks.10.norm1.weight', 'fusion_model.image_model.blocks.10.norm1.bias', 'fusion_model.image_model.blocks.10.attn.qkv.weight', 'fusion_model.image_model.blocks.10.attn.qkv.bias', 'fusion_model.image_model.blocks.10.attn.proj.weight', 'fusion_model.image_model.blocks.10.attn.proj.bias', 'fusion_model.image_model.blocks.10.norm2.weight', 'fusion_model.image_model.blocks.10.norm2.bias', 'fusion_model.image_model.blocks.10.mlp.fc1.weight', 'fusion_model.image_model.blocks.10.mlp.fc1.bias', 'fusion_model.image_model.blocks.10.mlp.fc2.weight', 'fusion_model.image_model.blocks.10.mlp.fc2.bias', 'fusion_model.image_model.blocks.11.norm1.weight', 'fusion_model.image_model.blocks.11.norm1.bias', 'fusion_model.image_model.blocks.11.attn.qkv.weight', 'fusion_model.image_model.blocks.11.attn.qkv.bias', 'fusion_model.image_model.blocks.11.attn.proj.weight', 'fusion_model.image_model.blocks.11.attn.proj.bias', 'fusion_model.image_model.blocks.11.norm2.weight', 'fusion_model.image_model.blocks.11.norm2.bias', 'fusion_model.image_model.blocks.11.mlp.fc1.weight', 'fusion_model.image_model.blocks.11.mlp.fc1.bias', 'fusion_model.image_model.blocks.11.mlp.fc2.weight', 'fusion_model.image_model.blocks.11.mlp.fc2.bias', 'fusion_model.image_model.blocks.12.norm1.weight', 'fusion_model.image_model.blocks.12.norm1.bias', 'fusion_model.image_model.blocks.12.attn.qkv.weight', 'fusion_model.image_model.blocks.12.attn.qkv.bias', 'fusion_model.image_model.blocks.12.attn.proj.weight', 'fusion_model.image_model.blocks.12.attn.proj.bias', 'fusion_model.image_model.blocks.12.norm2.weight', 'fusion_model.image_model.blocks.12.norm2.bias', 'fusion_model.image_model.blocks.12.mlp.fc1.weight', 'fusion_model.image_model.blocks.12.mlp.fc1.bias', 'fusion_model.image_model.blocks.12.mlp.fc2.weight', 'fusion_model.image_model.blocks.12.mlp.fc2.bias', 'fusion_model.image_model.blocks.13.norm1.weight', 'fusion_model.image_model.blocks.13.norm1.bias', 'fusion_model.image_model.blocks.13.attn.qkv.weight', 'fusion_model.image_model.blocks.13.attn.qkv.bias', 'fusion_model.image_model.blocks.13.attn.proj.weight', 'fusion_model.image_model.blocks.13.attn.proj.bias', 'fusion_model.image_model.blocks.13.norm2.weight', 'fusion_model.image_model.blocks.13.norm2.bias', 'fusion_model.image_model.blocks.13.mlp.fc1.weight', 'fusion_model.image_model.blocks.13.mlp.fc1.bias', 'fusion_model.image_model.blocks.13.mlp.fc2.weight', 'fusion_model.image_model.blocks.13.mlp.fc2.bias', 'fusion_model.image_model.blocks.14.norm1.weight', 'fusion_model.image_model.blocks.14.norm1.bias', 'fusion_model.image_model.blocks.14.attn.qkv.weight', 'fusion_model.image_model.blocks.14.attn.qkv.bias', 'fusion_model.image_model.blocks.14.attn.proj.weight', 'fusion_model.image_model.blocks.14.attn.proj.bias', 'fusion_model.image_model.blocks.14.norm2.weight', 'fusion_model.image_model.blocks.14.norm2.bias', 'fusion_model.image_model.blocks.14.mlp.fc1.weight', 'fusion_model.image_model.blocks.14.mlp.fc1.bias', 'fusion_model.image_model.blocks.14.mlp.fc2.weight', 'fusion_model.image_model.blocks.14.mlp.fc2.bias', 'fusion_model.image_model.blocks.15.norm1.weight', 'fusion_model.image_model.blocks.15.norm1.bias', 'fusion_model.image_model.blocks.15.attn.qkv.weight', 'fusion_model.image_model.blocks.15.attn.qkv.bias', 'fusion_model.image_model.blocks.15.attn.proj.weight', 'fusion_model.image_model.blocks.15.attn.proj.bias', 'fusion_model.image_model.blocks.15.norm2.weight', 'fusion_model.image_model.blocks.15.norm2.bias', 'fusion_model.image_model.blocks.15.mlp.fc1.weight', 'fusion_model.image_model.blocks.15.mlp.fc1.bias', 'fusion_model.image_model.blocks.15.mlp.fc2.weight', 'fusion_model.image_model.blocks.15.mlp.fc2.bias', 'fusion_model.image_model.blocks.16.norm1.weight', 'fusion_model.image_model.blocks.16.norm1.bias', 'fusion_model.image_model.blocks.16.attn.qkv.weight', 'fusion_model.image_model.blocks.16.attn.qkv.bias', 'fusion_model.image_model.blocks.16.attn.proj.weight', 'fusion_model.image_model.blocks.16.attn.proj.bias', 'fusion_model.image_model.blocks.16.norm2.weight', 'fusion_model.image_model.blocks.16.norm2.bias', 'fusion_model.image_model.blocks.16.mlp.fc1.weight', 'fusion_model.image_model.blocks.16.mlp.fc1.bias', 'fusion_model.image_model.blocks.16.mlp.fc2.weight', 'fusion_model.image_model.blocks.16.mlp.fc2.bias', 'fusion_model.image_model.blocks.17.norm1.weight', 'fusion_model.image_model.blocks.17.norm1.bias', 'fusion_model.image_model.blocks.17.attn.qkv.weight', 'fusion_model.image_model.blocks.17.attn.qkv.bias', 'fusion_model.image_model.blocks.17.attn.proj.weight', 'fusion_model.image_model.blocks.17.attn.proj.bias', 'fusion_model.image_model.blocks.17.norm2.weight', 'fusion_model.image_model.blocks.17.norm2.bias', 'fusion_model.image_model.blocks.17.mlp.fc1.weight', 'fusion_model.image_model.blocks.17.mlp.fc1.bias', 'fusion_model.image_model.blocks.17.mlp.fc2.weight', 'fusion_model.image_model.blocks.17.mlp.fc2.bias', 'fusion_model.image_model.blocks.18.norm1.weight', 'fusion_model.image_model.blocks.18.norm1.bias', 'fusion_model.image_model.blocks.18.attn.qkv.weight', 'fusion_model.image_model.blocks.18.attn.qkv.bias', 'fusion_model.image_model.blocks.18.attn.proj.weight', 'fusion_model.image_model.blocks.18.attn.proj.bias', 'fusion_model.image_model.blocks.18.norm2.weight', 'fusion_model.image_model.blocks.18.norm2.bias', 'fusion_model.image_model.blocks.18.mlp.fc1.weight', 'fusion_model.image_model.blocks.18.mlp.fc1.bias', 'fusion_model.image_model.blocks.18.mlp.fc2.weight', 'fusion_model.image_model.blocks.18.mlp.fc2.bias', 'fusion_model.image_model.blocks.19.norm1.weight', 'fusion_model.image_model.blocks.19.norm1.bias', 'fusion_model.image_model.blocks.19.attn.qkv.weight', 'fusion_model.image_model.blocks.19.attn.qkv.bias', 'fusion_model.image_model.blocks.19.attn.proj.weight', 'fusion_model.image_model.blocks.19.attn.proj.bias', 'fusion_model.image_model.blocks.19.norm2.weight', 'fusion_model.image_model.blocks.19.norm2.bias', 'fusion_model.image_model.blocks.19.mlp.fc1.weight', 'fusion_model.image_model.blocks.19.mlp.fc1.bias', 'fusion_model.image_model.blocks.19.mlp.fc2.weight', 'fusion_model.image_model.blocks.19.mlp.fc2.bias', 'fusion_model.image_model.blocks.20.norm1.weight', 'fusion_model.image_model.blocks.20.norm1.bias', 'fusion_model.image_model.blocks.20.attn.qkv.weight', 'fusion_model.image_model.blocks.20.attn.qkv.bias', 'fusion_model.image_model.blocks.20.attn.proj.weight', 'fusion_model.image_model.blocks.20.attn.proj.bias', 'fusion_model.image_model.blocks.20.norm2.weight', 'fusion_model.image_model.blocks.20.norm2.bias', 'fusion_model.image_model.blocks.20.mlp.fc1.weight', 'fusion_model.image_model.blocks.20.mlp.fc1.bias', 'fusion_model.image_model.blocks.20.mlp.fc2.weight', 'fusion_model.image_model.blocks.20.mlp.fc2.bias', 'fusion_model.image_model.blocks.21.norm1.weight', 'fusion_model.image_model.blocks.21.norm1.bias', 'fusion_model.image_model.blocks.21.attn.qkv.weight', 'fusion_model.image_model.blocks.21.attn.qkv.bias', 'fusion_model.image_model.blocks.21.attn.proj.weight', 'fusion_model.image_model.blocks.21.attn.proj.bias', 'fusion_model.image_model.blocks.21.norm2.weight', 'fusion_model.image_model.blocks.21.norm2.bias', 'fusion_model.image_model.blocks.21.mlp.fc1.weight', 'fusion_model.image_model.blocks.21.mlp.fc1.bias', 'fusion_model.image_model.blocks.21.mlp.fc2.weight', 'fusion_model.image_model.blocks.21.mlp.fc2.bias', 'fusion_model.image_model.blocks.22.norm1.weight', 'fusion_model.image_model.blocks.22.norm1.bias', 'fusion_model.image_model.blocks.22.attn.qkv.weight', 'fusion_model.image_model.blocks.22.attn.qkv.bias', 'fusion_model.image_model.blocks.22.attn.proj.weight', 'fusion_model.image_model.blocks.22.attn.proj.bias', 'fusion_model.image_model.blocks.22.norm2.weight', 'fusion_model.image_model.blocks.22.norm2.bias', 'fusion_model.image_model.blocks.22.mlp.fc1.weight', 'fusion_model.image_model.blocks.22.mlp.fc1.bias', 'fusion_model.image_model.blocks.22.mlp.fc2.weight', 'fusion_model.image_model.blocks.22.mlp.fc2.bias', 'fusion_model.image_model.blocks.23.norm1.weight', 'fusion_model.image_model.blocks.23.norm1.bias', 'fusion_model.image_model.blocks.23.attn.qkv.weight', 'fusion_model.image_model.blocks.23.attn.qkv.bias', 'fusion_model.image_model.blocks.23.attn.proj.weight', 'fusion_model.image_model.blocks.23.attn.proj.bias', 'fusion_model.image_model.blocks.23.norm2.weight', 'fusion_model.image_model.blocks.23.norm2.bias', 'fusion_model.image_model.blocks.23.mlp.fc1.weight', 'fusion_model.image_model.blocks.23.mlp.fc1.bias', 'fusion_model.image_model.blocks.23.mlp.fc2.weight', 'fusion_model.image_model.blocks.23.mlp.fc2.bias', 'fusion_model.image_model.head.weight', 'fusion_model.image_model.head.bias', 'fusion_model.image_model.fc_norm.weight', 'fusion_model.image_model.fc_norm.bias', 'fusion_model.tabular_model.0.weight', 'fusion_model.tabular_model.0.bias', 'fusion_model.tabular_model.3.weight', 'fusion_model.tabular_model.3.bias', 'fusion_model.fusion_model.0.weight', 'fusion_model.fusion_model.0.bias', 'fusion_model.fusion_model.3.weight', 'fusion_model.fusion_model.3.bias', 'fusion_model.fusion_model.6.weight', 'fusion_model.fusion_model.6.bias', 'fusion_model.fusion_model.9.weight', 'fusion_model.fusion_model.9.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load image and Tabular model\n",
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=1)\n",
    "\n",
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "\n",
    "# load fusion model\n",
    "tabular_model_fusion = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    ")\n",
    "\n",
    "image_model_fusion = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=8).requires_grad_(True)\n",
    "\n",
    "fusion_model_fusion = nn.Sequential(\n",
    "    nn.Linear(in_features=40, out_features=128),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=128, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")\n",
    "\n",
    "fusion = HyMNet(image_model=image_model_fusion, tabular_model=tabular_model_fusion, fusion_model=fusion_model_fusion)\n",
    "state_dict = torch.load(MODELS_PATH + \"/JointFusion_finetune.pth\")\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "fusion.load_state_dict(state_dict)\n",
    "\n",
    "model = HyMNet(image_model=image_model, tabular_model=tabular_model, fusion_model=fusion).to(device)\n",
    "\n",
    "state_dict = torch.load(FM_PATH)\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "state_dict = torch.load(DM_PATH)\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_tabular_dataset(model, train_dataset, device, method=\"vf\")\n",
    "test_x, test_y = build_tabular_dataset(model, test_dataset, device, method=\"vf\")\n",
    "\n",
    "train_fusion_set = InputOutputDataset(train_x, train_y)\n",
    "test_fusion_set = InputOutputDataset(test_x, test_y)\n",
    "\n",
    "train_fusion_loader = DataLoader(train_fusion_set, batch_size=1)\n",
    "test_fusion_loader = DataLoader(test_fusion_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.005, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODELS_PATH+'/VotingFusionXGBParams.json') as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "boost = xgb.XGBClassifier(**params, objective=\"binary:logistic\")\n",
    "\n",
    "boost.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: [0.718, 0.772] average: 0.745\n",
      "auroc: [0.671, 0.737] average: 0.704\n",
      "auprc: [0.712, 0.79] average: 0.751\n",
      "precision: [0.646, 0.715] average: 0.68\n",
      "recall: [0.792, 0.854] average: 0.823\n",
      "specificity: [0.393, 0.488] average: 0.44\n",
      "accuracy: [0.639, 0.696] average: 0.668\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=boost, data_loader=test_fusion_loader, save_loc=f\"{os.getcwd() + os.sep}Results/VotingFusionXGB.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, gamma=0.01, kernel='poly', probability=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODELS_PATH+ '/LateFusionSVMParams.json') as f:\n",
    "    params = json.load(f)\n",
    "        \n",
    "svm = SVC(**params, probability=True)\n",
    "\n",
    "svm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: [0.747, 0.794] average: 0.77\n",
      "auroc: [0.663, 0.73] average: 0.696\n",
      "auprc: [0.707, 0.786] average: 0.746\n",
      "precision: [0.613, 0.675] average: 0.644\n",
      "recall: [0.945, 0.976] average: 0.96\n",
      "specificity: [0.189, 0.269] average: 0.229\n",
      "accuracy: [0.634, 0.691] average: 0.662\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=svm, data_loader=test_fusion_loader, save_loc=f\"{os.getcwd() + os.sep}Results/VotingFusionSVM.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_path = MODELS_PATH + \"\\VotingFusionFCNN.pth\"\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "model = HyMNet(tabular_model=fusion_model).cuda().float()\n",
    "model.load_state_dict(torch.load(fm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1007/1007 [00:00<00:00, 2469.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: [0.713, 0.766] average: 0.74\n",
      "auroc: [0.648, 0.716] average: 0.682\n",
      "auprc: [0.696, 0.775] average: 0.736\n",
      "precision: [0.64, 0.708] average: 0.674\n",
      "recall: [0.789, 0.851] average: 0.82\n",
      "specificity: [0.378, 0.474] average: 0.426\n",
      "accuracy: [0.63, 0.688] average: 0.659\n"
     ]
    }
   ],
   "source": [
    "results = boostrap(N=N, model=model, data_loader=test_fusion_loader, save_loc=f\"{os.getcwd() + os.sep}Results/VotingFusionFCNN.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = nn.Sigmoid()\n",
    "\n",
    "tensored_x, tensored_y = torch.tensor(test_x), torch.tensor(test_y)\n",
    "probs = sig(tensored_x)\n",
    "\n",
    "average = torch.mean(probs, axis=1)\n",
    "\n",
    "esnemble_average = torch.stack([average, tensored_y], axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: [0.741, 0.791] average: 0.766\n",
      "auroc: [0.676, 0.741] average: 0.708\n",
      "auprc: [0.717, 0.796] average: 0.756\n",
      "precision: [0.646, 0.712] average: 0.679\n",
      "recall: [0.854, 0.906] average: 0.88\n",
      "specificity: [0.349, 0.444] average: 0.396\n",
      "accuracy: [0.654, 0.711] average: 0.682\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(tensored_x)\n",
    "indices = [i for i in range(n_samples)]\n",
    "\n",
    "bootstrap_preds, bootstrap_trues = [], []\n",
    "for run in range(N):\n",
    "    bootstrapped_indices = resample(indices, replace=True, n_samples=n_samples)\n",
    "    preds_tmp , trues_tmp = [average[indx] for indx in bootstrapped_indices], [tensored_y[indx] for indx in bootstrapped_indices]\n",
    "    bootstrap_preds.append(preds_tmp)\n",
    "    bootstrap_trues.append(trues_tmp)\n",
    "\n",
    "results = calculate_results(N, bootstrap_trues, bootstrap_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    ")\n",
    "\n",
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=8).requires_grad_(True)\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=40, out_features=128),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=128, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")\n",
    "\n",
    "model = HyMNet(image_model=image_model, tabular_model=tabular_model, fusion_model=fusion_model).to(device)\n",
    "state_dict = torch.load(MODELS_PATH + \"/JointFusion_finetune.pth\")\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baharoon/HTN/HyMNet/Utils/Utils.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['HTN_DM'] = df['HTN'].astype(str) + df['DM'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = HypertensionDataset(CSV_PATH, split=\"test\", test_transform=test_transform)\n",
    "test_dataset.df = test_dataset.df[test_dataset.df[\"DM\"] == 1]\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "849"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = boostrap(N=N, model=model, data_loader=test_loader)\n",
    "with open(os.getcwd() + os.sep + \"Results/JointFusion_dm_finetune.json\", 'w') as f:\n",
    "    # Use json.dump to write the dictionary to the file\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baharoon/HTN/HyMNet/Utils/Utils.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['HTN_DM'] = df['HTN'].astype(str) + df['DM'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = HypertensionDataset(CSV_PATH, split=\"test\", test_transform=test_transform)\n",
    "test_dataset.df = test_dataset.df[test_dataset.df[\"DM\"] == 0]\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = boostrap(N=N, model=model, data_loader=test_loader)\n",
    "with open(os.getcwd() + os.sep + \"Results/JointFusion_nodm_finetune.json\", 'w') as f:\n",
    "    # Use json.dump to write the dictionary to the file\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 14x14 to 32x32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    ")\n",
    "\n",
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=8).requires_grad_(True)\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=40, out_features=128),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=128, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")\n",
    "\n",
    "model1 = HyMNet(image_model=image_model, tabular_model=tabular_model, fusion_model=fusion_model).to(device)\n",
    "state_dict = torch.load(MODELS_PATH + \"/JointFusion_finetune.pth\")\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model1.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ").to(device)\n",
    "\n",
    "model2 = HyMNet(tabular_model=tabular_model) \n",
    "model2.load_state_dict(torch.load(MODELS_PATH + \"/DemographicFCNN.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1007/1007 [02:30<00:00,  6.68it/s]\n"
     ]
    }
   ],
   "source": [
    "results = binary_bootstrap(model1, model2, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "for m in range(1, 3):\n",
    "    f1_scores = []\n",
    "    for i in range(N):\n",
    "        preds = [1 if prob >= 0.5 else 0 for prob in results[f'prob{m}'][i]]\n",
    "        f1_scores.append(sklearn.metrics.f1_score(results['true'][i], preds))   \n",
    "    model_scores[f'score{m}'] = f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = np.array(model_scores['score1'])\n",
    "score2 = np.array(model_scores['score2'])\n",
    "difference = score1 - score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference.sort()\n",
    "sig_dif = difference[(difference >= np.percentile(difference, 2.5)) & \\\n",
    "                                     (difference <= np.percentile(difference, 97.5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013227980672246595\n",
      "-0.005257952001666011\n",
      "0.03223390357970779\n"
     ]
    }
   ],
   "source": [
    "print(sig_dif.mean())\n",
    "print(sig_dif.min())\n",
    "print(sig_dif.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 14x14 to 32x32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    ")\n",
    "\n",
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=8).requires_grad_(True)\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=40, out_features=128),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=128, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")\n",
    "\n",
    "model = HyMNet(image_model=image_model, tabular_model=tabular_model, fusion_model=fusion_model).to(device)\n",
    "state_dict = torch.load(MODELS_PATH + \"/JointFusion_finetune.pth\")\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "for sample in tqdm(test_loader):\n",
    "        \n",
    "    img, features, target = sample[\"image\"], sample[\"features\"], sample[\"label\"]\n",
    "    img, features, target = img.to(device).float(), features.to(device).float(),\\\n",
    "                                    target.to(device).float()\n",
    "    \n",
    "    output_image = model.image_model(img).squeeze(0)\n",
    "    output_features = model.tabular_model(features).squeeze(0)\n",
    "    \n",
    "    concated = torch.cat([output_image, output_features], dim=0)\n",
    "    \n",
    "    test_x.append(concated.detach().cpu().numpy())\n",
    "    test_y.append(target.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.stack(test_x)\n",
    "test_y = np.stack(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.tensor(test_x, device=device)\n",
    "test_y = torch.tensor(test_y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func(data):\n",
    "    # Set the model in evaluation mode and disable gradients\n",
    "    fusion_model.eval().cpu()\n",
    "    with torch.no_grad():\n",
    "#         data = torch.tensor(data, device=device)\n",
    "        data = torch.tensor(data)\n",
    "        predictions = fusion_model(data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.GradientExplainer(fusion_model, test_x[0: ,].cpu().unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(test_x[0: ,].cpu().unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retfound",
   "language": "python",
   "name": "retfound"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
