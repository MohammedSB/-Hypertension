{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run C:/Users/MohammedSB/Desktop/projects/Hypertension/Requirements.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"D:\\\\MohammedSB\\\\Fundus\\\\\"\n",
    "CSV_PATH = {\"HTNPath\": PATH + r\"HTN\", \"NonHTNPath\": PATH + \"NonHTN\"}\n",
    "\n",
    "MODELS_PATH = r\"C:\\Users\\MohammedSB\\Desktop\\projects\\Hypertension\\models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((512, 512)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.3675, 0.1760, 0.1047], std=[0.2289, 0.1180, 0.0748]),\n",
    "])\n",
    "\n",
    "train_dataset = HypertensionDataset(CSV_PATH, train_set, train_transform=transform)\n",
    "test_dataset = HypertensionDataset(CSV_PATH, test_set, test_transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(N, trues, probs, threshold=0.5):\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "    pr_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    specificity_scores = []\n",
    "\n",
    "    for i in range(N):\n",
    "        fpr, tpr, _ = roc_curve(trues[i], probs[i])\n",
    "        precision, recall, _ = precision_recall_curve(trues[i], probs[i])\n",
    "\n",
    "        \n",
    "        f1_scores.append(f1_score(trues[i], np.array(probs[i])>=threshold))\n",
    "        auc_scores.append(auc(fpr, tpr))\n",
    "        pr_scores.append(auc(recall, precision))\n",
    "        precision_scores.append(precision_score(trues[i], np.array(probs[i])>=threshold))\n",
    "        recall_scores.append(recall_score(trues[i], np.array(probs[i])>=threshold))\n",
    "        specificity_scores.append(recall_score(trues[i], np.array(probs[i])>=threshold, pos_label=0))\n",
    "    \n",
    "    print(f\"f1: {np.percentile(f1_scores, [2.5, 97.5])}\") \n",
    "    print(f\"auroc: {np.percentile(auc_scores, [2.5, 97.5])}\")\n",
    "    print(f\"auprc: {np.percentile(pr_scores, [2.5, 97.5])}\") \n",
    "    print(f\"precision: {np.percentile(precision_scores, [2.5, 97.5])}\") \n",
    "    print(f\"recall: {np.percentile(recall_scores, [2.5, 97.5])}\") \n",
    "    print(f\"specificity: {np.percentile(specificity_scores, [2.5, 97.5])}\")\n",
    "    \n",
    "    \n",
    "def bootstrap(N, models, data_set, device, criterion=None, method=\"torch\"):\n",
    "    num_samples = len(data_set)\n",
    "    trues, probs = dict(), dict()\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "\n",
    "        if method == \"torch\":\n",
    "            sampler = torch.utils.data.RandomSampler(data_set, replacement=True, num_samples=num_samples)\n",
    "            data_loader = DataLoader(data_set, batch_size=8, sampler=sampler)\n",
    "\n",
    "            metrics = test(models=models, criterion=criterion, test_loader=data_loader, device=device, show_output=False)\n",
    "            y_true = metrics[\"y_true\"].squeeze(1).numpy()\n",
    "            y_prob = metrics[\"y_prob\"].squeeze(1).numpy()\n",
    "\n",
    "            trues[i] = y_true.tolist()\n",
    "            probs[i] = y_prob.tolist()\n",
    "        elif method == \"sklearn\":\n",
    "            bootstrapped_data = resample(data_set, replace=True, n_samples=num_samples)\n",
    "            x, y = bootstrapped_data[:, :-1], bootstrapped_data[:, -1]\n",
    "            probas = boost.predict_proba(x)[:, 1]\n",
    "            \n",
    "            trues[i] = y.tolist()\n",
    "            probs[i] = probas.tolist()\n",
    "    \n",
    "    return trues, probs\n",
    "\n",
    "\n",
    "def bootstrap_session(N, models, data_set, device, criterion, show_results=True, method=\"torch\", loc=None):\n",
    "    \n",
    "    results = dict()\n",
    "    \n",
    "    trues, probs = bootstrap(N, models, data_set, device, criterion, method=method)\n",
    "\n",
    "    results[\"true\"] = trues\n",
    "    results[\"prob\"] = probs\n",
    "    \n",
    "    trues, probs = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        trues += results[\"true\"][i]\n",
    "        probs += results[\"prob\"][i]\n",
    "    \n",
    "    if show_results:\n",
    "        print(\"Results:\")\n",
    "        calculate_results(N, results['true'], results['prob'], 0.5) \n",
    "    \n",
    "    if loc is not None:\n",
    "        with open(loc+\".json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        return trues, probs\n",
    "\n",
    "def optimal_threshold(trues, probs):\n",
    "    precision, recall, thresholds = precision_recall_curve(trues, probs)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    ix = np.argmax(fscore)\n",
    "    print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "    return thresholds[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "\n",
    "FM_PATH = MODELS_PATH + r\"\\FundusModel.pth\"\n",
    "DM_PATH = MODELS_PATH + r\"\\DemographicFCNN.pth\"\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FundusModel Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_model = get_densenet201(device=device, freeze=True, with_mlp=True)\n",
    "image_model.load_state_dict(torch.load(FM_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"image_model\": image_model,\n",
    "    \"tabular_model\": None,\n",
    "    \"fusion_model\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [2:02:32<00:00,  7.35s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.6615     0.77359513]\n",
      "auroc: [0.67111445 0.79618103]\n",
      "auprc: [0.62529124 0.80026757]\n",
      "precision: [0.51036811 0.647393  ]\n",
      "recall: [0.91596442 0.99082569]\n",
      "specificity: [0.23076923 0.39827434]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=models, data_set=test_dataset, device=device, criterion=criterion, loc='Results/FundusModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DemographicModels Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train_set[[\"Age\", \"Gender\"]], train_set[\"Hypertension\"]\n",
    "test_x, test_y = test_set[[\"Age\", \"Gender\"]], test_set[\"Hypertension\"]\n",
    "\n",
    "combined_test = np.hstack([test_x, np.expand_dims(test_y, -1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.0001, max_bin=256,\n",
       "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "              max_depth=3, max_leaves=0, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODELS_PATH+'/DemographicXGBParams.json') as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "boost = xgb.XGBClassifier(**params, objective=\"binary:logistic\")\n",
    "\n",
    "boost.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 750.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.67652817 0.79554492]\n",
      "auroc: [0.67893514 0.80429802]\n",
      "auprc: [0.53123468 0.76458057]\n",
      "precision: [0.56521033 0.71796875]\n",
      "recall: [0.80670025 0.92594292]\n",
      "specificity: [0.42590278 0.61389947]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=boost, data_set=combined_test, device=device, criterion=criterion,\\\n",
    "                  method=\"sklearn\", loc='Results/DemographicXGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, gamma=0.01)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODELS_PATH+'/DemographicSVMParams.json') as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "svm = SVC(**params)\n",
    "\n",
    "svm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 758.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.67435976 0.78985507]\n",
      "auroc: [0.67441774 0.80183051]\n",
      "auprc: [0.52511126 0.76481834]\n",
      "precision: [0.56081081 0.71523294]\n",
      "recall: [0.80356411 0.92481673]\n",
      "specificity: [0.43112713 0.60749805]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=svm, data_set=combined_test, device=device, criterion=criterion, \\\n",
    "                  method=\"sklearn\", loc='Results/DemographicSVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "tabular_model = tabular_model.to(device)\n",
    "tabular_model.load_state_dict(torch.load(DM_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"image_model\": None,\n",
    "    \"tabular_model\": tabular_model,\n",
    "    \"fusion_model\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:16:36<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.67703981 0.79054447]\n",
      "auroc: [0.69891585 0.81851042]\n",
      "auprc: [0.64978408 0.81064488]\n",
      "precision: [0.55487805 0.69812833]\n",
      "recall: [0.83177339 0.94644042]\n",
      "specificity: [0.38786391 0.5714591 ]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=models, data_set=test_dataset, device=device, criterion=criterion, loc='Results/DemographicFCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FusionModel Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CombineFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_path = MODELS_PATH + r\"\\CombineFeatures\\DemographicPath.pth\"\n",
    "img_path = MODELS_PATH + r\"\\CombineFeatures\\FundusPath.pth\"\n",
    "fm_path = MODELS_PATH + r\"\\CombineFeatures\\FusionPath.pth\"\n",
    "\n",
    "\n",
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    ")\n",
    "\n",
    "image_model = get_densenet201(device=device, freeze=True, with_mlp=False, outputs=32)\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=64, out_features=128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=128, out_features=32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")\n",
    "\n",
    "tabular_model, image_model, fusion_model = tabular_model.to(device).float(), image_model.to(device).float(),\\\n",
    "                                                fusion_model.to(device).float()\n",
    "\n",
    "tabular_model.load_state_dict(torch.load(tm_path))\n",
    "image_model.load_state_dict(torch.load(img_path))\n",
    "fusion_model.load_state_dict(torch.load(fm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"image_model\": image_model,\n",
    "    \"tabular_model\": tabular_model,\n",
    "    \"fusion_model\": fusion_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [2:01:57<00:00,  7.32s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.66922598 0.78723404]\n",
      "auroc: [0.7205815  0.83794064]\n",
      "auprc: [0.66107787 0.83009923]\n",
      "precision: [0.54164701 0.68824498]\n",
      "recall: [0.84402953 0.95412844]\n",
      "specificity: [0.35041667 0.53508772]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=models, data_set=test_dataset, device=device, criterion=criterion, loc='Results/CombineFeatures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CombineOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_path = MODELS_PATH + r\"\\CombineOutputs\\DemographicPath.pth\"\n",
    "img_path = MODELS_PATH + r\"\\CombineOutputs\\FundusPath.pth\"\n",
    "fm_path = MODELS_PATH + r\"\\CombineOutputs\\FusionPath.pth\"\n",
    "\n",
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "image_model = get_densenet201(device=device, freeze=True, with_mlp=True, outputs=1)\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=16, out_features=4),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=4, out_features=1),\n",
    ")\n",
    "\n",
    "tabular_model, image_model, fusion_model = tabular_model.to(device).float(), image_model.to(device).float(),\\\n",
    "                                                fusion_model.to(device).float()\n",
    "\n",
    "tabular_model.load_state_dict(torch.load(tm_path))\n",
    "image_model.load_state_dict(torch.load(img_path))\n",
    "fusion_model.load_state_dict(torch.load(fm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"image_model\": image_model,\n",
    "    \"tabular_model\": tabular_model,\n",
    "    \"fusion_model\": fusion_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [2:02:03<00:00,  7.32s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.66382979 0.78791362]\n",
      "auroc: [0.72139707 0.83419071]\n",
      "auprc: [0.65764018 0.8358386 ]\n",
      "precision: [0.57040038 0.72222222]\n",
      "recall: [0.77226854 0.90656369]\n",
      "specificity: [0.45967197 0.63493547]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=models, data_set=test_dataset, device=device, criterion=criterion, loc='Results/CombineOutputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeepFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = get_densenet201(device=device, freeze=True, with_mlp=True)\n",
    "image_model.load_state_dict(torch.load(FM_PATH))\n",
    "\n",
    "image_model = image_model.to(device).float()\n",
    "\n",
    "models = {\n",
    "    \"image_model\": image_model,\n",
    "    \"tabular_model\": None,\n",
    "    \"fusion_model\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_tabular_dataset(models, train_dataset, method=\"keep_features\")\n",
    "\n",
    "test_x, test_y = build_tabular_dataset(models, test_dataset, method=\"keep_features\")\n",
    "test_fusion_set = InputOutputDataset(test_x, test_y)\n",
    "\n",
    "combined_test = np.hstack([test_x, np.expand_dims(test_y, -1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.005, max_bin=256,\n",
       "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "              max_depth=3, max_leaves=0, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=1000, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODELS_PATH+ '/KeepFeatures/XGBParams.json') as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "boost = xgb.XGBClassifier(**params, objective=\"binary:logistic\")\n",
    "\n",
    "boost.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 556.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.63114663 0.76116187]\n",
      "auroc: [0.6397735  0.77288224]\n",
      "auprc: [0.55877038 0.76210067]\n",
      "precision: [0.54725123 0.71853501]\n",
      "recall: [0.70243872 0.85454934]\n",
      "specificity: [0.46210664 0.643493  ]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=boost, data_set=combined_test, device=device, criterion=criterion,\\\n",
    "                  method=\"sklearn\", loc='Results/KeepFeaturesXGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, gamma=0.01)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODELS_PATH+ '/KeepFeatures/SVMParams.json') as f:\n",
    "    params = json.load(f)\n",
    "        \n",
    "svm = SVC(**params)\n",
    "\n",
    "svm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 563.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.63708864 0.75410711]\n",
      "auroc: [0.64403184 0.77217561]\n",
      "auprc: [0.56759036 0.75584324]\n",
      "precision: [0.56023008 0.71094575]\n",
      "recall: [0.70191886 0.8500463 ]\n",
      "specificity: [0.47271465 0.64547203]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=svm, data_set=combined_test, device=device, criterion=criterion,\\\n",
    "                  method=\"sklearn\", loc='Results/KeepFeaturesSVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_path = MODELS_PATH + r\"\\KeepFeatures\\FusionPath.pth\"\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "fusion_model = fusion_model.to(device).float()\n",
    "fusion_model.load_state_dict(torch.load(fm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"image_model\": None,\n",
    "    \"tabular_model\": None,\n",
    "    \"fusion_model\": fusion_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.67692015 0.789143  ]\n",
      "auroc: [0.68774533 0.81447842]\n",
      "auprc: [0.6427793  0.81375371]\n",
      "precision: [0.55553763 0.70069573]\n",
      "recall: [0.82856421 0.94392523]\n",
      "specificity: [0.40350509 0.57549932]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=models, data_set=test_fusion_set, device=device, criterion=criterion,\\\n",
    "                  loc='Results/KeepFeaturesFCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and Tabular model\n",
    "image_model = get_densenet201(device=device, freeze=True, with_mlp=True, outputs=1)\n",
    "\n",
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "image_model.load_state_dict(torch.load(FM_PATH))\n",
    "tabular_model.load_state_dict(torch.load(DM_PATH))\n",
    "\n",
    "image_model, tabular_model = image_model.to(device).float(), tabular_model.to(device).float()\n",
    "\n",
    "# load fusion model\n",
    "tabular_path = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    ")\n",
    "\n",
    "image_path = get_densenet201(device=device, freeze=True, with_mlp=False, outputs=32)\n",
    "\n",
    "fusion_path = nn.Sequential(\n",
    "    nn.Linear(in_features=64, out_features=128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=128, out_features=32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")\n",
    "\n",
    "image_path.load_state_dict(torch.load(MODELS_PATH + \"\\CombineFeatures\\FundusPath.pth\"))\n",
    "tabular_path.load_state_dict(torch.load(MODELS_PATH + \"\\CombineFeatures\\DemographicPath.pth\"))\n",
    "fusion_path.load_state_dict(torch.load(MODELS_PATH + \"\\CombineFeatures\\FusionPath.pth\"))\n",
    "\n",
    "tabular_path, image_path, fusion_path = tabular_path.to(device).float(), image_path.to(device).float(),\\\n",
    "                                                fusion_path.to(device).float()\n",
    "\n",
    "fusion_model = dict()\n",
    "fusion_model[\"image_path\"], fusion_model[\"tabular_path\"], fusion_model[\"fusion_path\"] = image_path, \\\n",
    "                                                                                    tabular_path, fusion_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"image_model\": image_model,\n",
    "    \"tabular_model\": tabular_model,\n",
    "    \"fusion_model\": fusion_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_tabular_dataset(models, train_dataset, method=\"voting_features\")\n",
    "\n",
    "test_x, test_y = build_tabular_dataset(models, test_dataset, method=\"voting_features\")\n",
    "test_fusion_set = InputOutputDataset(test_x, test_y)\n",
    "\n",
    "combined_test = np.hstack([test_x, np.expand_dims(test_y, -1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.3,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.005, max_bin=256,\n",
       "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "              max_depth=3, max_leaves=0, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=500, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODELS_PATH+'/VotingFeatures/XGBParams.json') as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "boost = xgb.XGBClassifier(**params, objective=\"binary:logistic\")\n",
    "\n",
    "boost.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 610.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.66406094 0.78549676]\n",
      "auroc: [0.70219437 0.82188266]\n",
      "auprc: [0.67925741 0.83085986]\n",
      "precision: [0.5443038  0.69187165]\n",
      "recall: [0.8204955  0.94285714]\n",
      "specificity: [0.38278764 0.55557971]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=boost, data_set=combined_test, device=device, criterion=criterion,\\\n",
    "                  method=\"sklearn\", loc='Results/VotingFeaturesXGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000, gamma=0.01)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(MODELS_PATH+ '/KeepFeatures/SVMParams.json') as f:\n",
    "    params = json.load(f)\n",
    "        \n",
    "svm = SVC(**params)\n",
    "\n",
    "svm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 601.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.66423221 0.78447735]\n",
      "auroc: [0.70300574 0.82762664]\n",
      "auprc: [0.67684662 0.83412412]\n",
      "precision: [0.5443038  0.70001497]\n",
      "recall: [0.82257688 0.94059616]\n",
      "specificity: [0.378767   0.56757329]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=svm, data_set=combined_test, device=device, criterion=criterion,\\\n",
    "                  method=\"sklearn\", loc='Results/VotingFeaturesSVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_path = MODELS_PATH + \"\\VotingFeatures\\FusionPath.pth\"\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "fusion_model = fusion_model.to(device).float()\n",
    "fusion_model.load_state_dict(torch.load(fm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"image_model\": None,\n",
    "    \"tabular_model\": None,\n",
    "    \"fusion_model\": fusion_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "f1: [0.66666667 0.78351885]\n",
      "auroc: [0.66813748 0.79278959]\n",
      "auprc: [0.59462648 0.78277043]\n",
      "precision: [0.53888841 0.68629556]\n",
      "recall: [0.83783206 0.94599295]\n",
      "specificity: [0.35714286 0.54241005]\n"
     ]
    }
   ],
   "source": [
    "bootstrap_session(N, models=models, data_set=test_fusion_set, device=device, criterion=criterion,\\\n",
    "                  loc='Results/VotingFeaturesFCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = nn.Sigmoid()\n",
    "\n",
    "tensored_x, tensored_y = torch.tensor(test_x), torch.tensor(test_y)\n",
    "probs = sig(tensored_x)\n",
    "\n",
    "average = torch.mean(probs, axis=1)\n",
    "\n",
    "esnemble_average = torch.stack([average, tensored_y], axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 4784.96it/s]\n"
     ]
    }
   ],
   "source": [
    "trues, probs = dict(), dict()\n",
    "results = dict()\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    bootstrapped_data = resample(esnemble_average, replace=True, n_samples=len(esnemble_average))\n",
    "    x, y = bootstrapped_data[:, :-1], bootstrapped_data[:, -1]\n",
    "    \n",
    "    trues[i] = y.tolist()\n",
    "    probs[i] = x.tolist()\n",
    "    \n",
    "results[\"true\"] = trues\n",
    "results[\"prob\"] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Results/VotingFeaturesEnsemble.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 133934.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: [0.66411908 0.78500122]\n",
      "auroc: [0.64482753 0.7766236 ]\n",
      "auprc: [0.5816743  0.77394475]\n",
      "precision: [0.53626981 0.68837662]\n",
      "recall: [0.83193091 0.94646351]\n",
      "specificity: [0.3627451  0.54205607]\n"
     ]
    }
   ],
   "source": [
    "trues, probs = [], []\n",
    "\n",
    "for i in range(N):\n",
    "    trues += results[\"true\"][i]\n",
    "    probs += results[\"prob\"][i]\n",
    "    \n",
    "calculate_results(N, results['true'], results['prob'], 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
