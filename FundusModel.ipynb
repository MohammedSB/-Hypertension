{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Utils import *\n",
    "from Utils.Blacksmith import * \n",
    "\n",
    "from Utils.HyMNet import HyMNet\n",
    "from timm.models.layers import trunc_normal_\n",
    "import Utils.ViT as vit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/baharoon/HTN/data/\"\n",
    "CSV_PATH = {\"HTNPath\": PATH + r\"HTN\", \"NonHTNPath\": PATH + \"NonHTN\"}\n",
    "\n",
    "MODELS_PATH = \"/home/baharoon/HTN/Models\"\n",
    "\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((586, 586)),\n",
    "    T.CenterCrop(512),\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "    T.RandomRotation(degrees=(0, 360)),\n",
    "    T.GaussianBlur(3),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((586, 586)),\n",
    "    T.CenterCrop(512),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# train_transform = T.Compose([\n",
    "#     T.Resize((256, 256)),\n",
    "#     T.CenterCrop(224),\n",
    "#     T.ToTensor(),\n",
    "#     T.RandomHorizontalFlip(0.5),\n",
    "#     T.RandomRotation(degrees=(0, 360)),\n",
    "#     T.GaussianBlur(3),\n",
    "#     T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# test_transform = T.Compose([\n",
    "#     T.Resize((256, 256)),\n",
    "#     T.CenterCrop(224),\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "train_dataset = HypertensionDataset(CSV_PATH, split=\"train\", train_transform=train_transform)\n",
    "val_dataset = HypertensionDataset(CSV_PATH, split=\"val\", test_transform=train_transform)\n",
    "test_dataset = HypertensionDataset(CSV_PATH, split=\"test\", test_transform=test_transform)\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1e-6]\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for lr in lrs:\n",
    "     \n",
    "    image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", classes=1, image_size=512).requires_grad_(True)\n",
    "    \n",
    "    model = HyMNet(image_model=image_model)\n",
    "\n",
    "    all_params = dict(model.image_model.named_parameters())\n",
    "    head_params = dict(model.image_model.head.named_parameters())\n",
    "\n",
    "    del all_params['head.weight']\n",
    "    del all_params['head.bias']\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": head_params.values(), \"lr\": 0.005},\n",
    "        {\"params\": all_params.values(), \"lr\": lr}\n",
    "    ])\n",
    "\n",
    "    # Parallelize model to multiple GPUs\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Send the model to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    epochs = 50 \n",
    "    epoch_length = math.ceil(len(train_dataset) / BATCH_SIZE)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epoch_length * epochs, eta_min=0)\n",
    "    metrics, bm = train_val(epochs=epochs, model=model, criterion=criterion, optimizer=optimizer, train_loader=train_loader,\n",
    "                        val_loader=test_loader, scheduler=scheduler, device=device, save_model=True)\n",
    "    results[lr] = max([metrics[1][i][\"AUROC\"] for i in metrics[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/baharoon/HTN/HyMNet/Results/retfound_224.json\", 'w') as f:\n",
    "    # Use json.dump to write the dictionary to the file\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bm, MODELS_PATH + r'/Retfound.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retfound",
   "language": "python",
   "name": "retfound"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
