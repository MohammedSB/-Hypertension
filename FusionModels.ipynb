{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Utils import *\n",
    "from Utils.Blacksmith import * \n",
    "\n",
    "from Utils.HyMNet import HyMNet\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/baharoon/HTN/data/\"\n",
    "CSV_PATH = {\"HTNPath\": PATH + r\"HTN\", \"NonHTNPath\": PATH + \"NonHTN\"}\n",
    "\n",
    "MODELS_PATH = \"/home/baharoon/HTN/Models\"\n",
    "\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "epochs = 50\n",
    "\n",
    "image_size = 586\n",
    "crop_size = 512\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((image_size, image_size)),\n",
    "    T.CenterCrop(crop_size),\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "    T.RandomRotation(degrees=(0, 360)),\n",
    "    T.GaussianBlur(3),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((image_size, image_size)),\n",
    "    T.CenterCrop(crop_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = HypertensionDataset(CSV_PATH, split=\"train\", train_transform=train_transform)\n",
    "# val_dataset = HypertensionDataset(CSV_PATH, split=\"val\", test_transform=test_transform)\n",
    "val_dataset = HypertensionDataset(CSV_PATH, split=\"val\", test_transform=train_transform)\n",
    "test_dataset = HypertensionDataset(CSV_PATH, split=\"test\", test_transform=test_transform)\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "epoch_length = math.ceil(len(train_dataset) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_PATH = MODELS_PATH + r\"/FundusModel.pth\"\n",
    "DM_PATH = MODELS_PATH + r\"/DemographicFCNN.pth\"\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatureFusion (Joint Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    ")\n",
    "\n",
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=8).requires_grad_(True)\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=40, out_features=128),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=128, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HyMNet(image_model=image_model, tabular_model=tabular_model, fusion_model=fusion_model)\n",
    "\n",
    "all_params_image = dict(model.image_model.named_parameters())\n",
    "head_params_image = dict(model.image_model.head.named_parameters())\n",
    "\n",
    "del all_params_image['head.weight']\n",
    "del all_params_image['head.bias']\n",
    "\n",
    "# Parallelize model to multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': all_params_image.values(), 'lr': 1e-6},\n",
    "    {'params': head_params_image.values(), 'lr': 0.005},\n",
    "    {'params': tabular_model.parameters(), 'lr': 0.005},\n",
    "    {'params': fusion_model.parameters(), 'lr': 0.005},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "epoch_length = math.ceil(len(train_dataset) / BATCH_SIZE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epoch_length * epochs, eta_min=0)\n",
    "metrics, best_model = train_val(epochs=epochs, model=model, criterion=criterion, optimizer=optimizer, train_loader=train_loader,\n",
    "                        val_loader=test_loader, scheduler=scheduler, device=device, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, MODELS_PATH+ \"/JointFusion_finetune.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PredictionFusion (Late Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=1).requires_grad_(True)\n",
    "\n",
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = HyMNet(image_model=image_model, tabular_model=tabular_model, fusion_model=fusion_model)\n",
    "\n",
    "all_params_image = dict(model.image_model.named_parameters())\n",
    "head_params_image = dict(model.image_model.head.named_parameters())\n",
    "\n",
    "del all_params_image['head.weight']\n",
    "del all_params_image['head.bias']\n",
    "\n",
    "# Parallelize model to multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': all_params_image.values(), 'lr': 1e-6},\n",
    "    {'params': head_params_image.values(), 'lr': 0.005},\n",
    "    {'params': tabular_model.parameters(), 'lr': 0.005},\n",
    "    {'params': fusion_model.parameters(), 'lr': 0.005},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "epoch_length = math.ceil(len(train_dataset) / BATCH_SIZE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epoch_length * epochs, eta_min=0)\n",
    "metrics, best_model = train_val(epochs=epochs, model=model, criterion=criterion, optimizer=optimizer, train_loader=train_loader,\n",
    "                        val_loader=test_loader, scheduler=scheduler, device=device, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, MODELS_PATH+ \"/PredFusion.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LateFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=1).requires_grad_(True)\n",
    "model = HyMNet(image_model=image_model).to(device)\n",
    "state_dict = torch.load(FM_PATH)\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_tabular_dataset(model, train_dataset, method=\"lf\")\n",
    "test_x, test_y = build_tabular_dataset(model, test_dataset, method=\"lf\")\n",
    "\n",
    "train_fusion_set = InputOutputDataset(train_x, train_y)\n",
    "test_fusion_set = InputOutputDataset(test_x, test_y)\n",
    "\n",
    "train_fusion_loader = DataLoader(train_fusion_set, batch_size=16)\n",
    "test_fusion_loader = DataLoader(test_fusion_set, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = xgb.XGBClassifier(tree_method='gpu_hist', objective=\"binary:logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [3,6,10],\n",
    "           'learning_rate': [0.0001, 0.005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.3],\n",
    "           'n_estimators': [100, 500, 1000],\n",
    "           'colsample_bytree': [0.3, 0.7]}\n",
    "\n",
    "clf = GridSearchCV(estimator=boost, \n",
    "                   param_grid=param_grid,\n",
    "                   scoring='roc_auc', \n",
    "                   verbose=1)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODELS_PATH+'/LateFusionXGBParams.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(clf.best_params_, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "    {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']},\n",
    "    {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001], 'degree': [1,2,3], 'kernel': ['poly']}\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(estimator=svm, \n",
    "                   param_grid=param_grid,\n",
    "                   scoring='roc_auc', \n",
    "                   verbose=1)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODELS_PATH+'/LateFusionSVMParams.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(clf.best_params_, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "fusion_model = fusion_model.to(device).float()\n",
    "model = HyMNet(tabular_model=fusion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': fusion_model.parameters(), 'lr': 0.005}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 250\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epoch_length * epochs, eta_min=0)\n",
    "metrics, best_model = train_val(epochs=epochs, model=model, criterion=criterion, optimizer=optimizer, train_loader=train_fusion_loader,\n",
    "                    val_loader=test_fusion_loader, scheduler=scheduler, device=device, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, MODELS_PATH + \"\\LateFusionFCNN.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingFusion (Late Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and Tabular model\n",
    "image_model = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=1)\n",
    "\n",
    "tabular_model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "\n",
    "# load fusion model\n",
    "tabular_model_fusion = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    ")\n",
    "\n",
    "image_model_fusion = get_retfound(\"/home/baharoon/HTN/RETFound_cfp_weights.pth\", image_size=512,\n",
    "                          classes=8).requires_grad_(True)\n",
    "\n",
    "fusion_model_fusion = nn.Sequential(\n",
    "    nn.Linear(in_features=40, out_features=128),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=128, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=1),\n",
    ")\n",
    "\n",
    "fusion = HyMNet(image_model=image_model_fusion, tabular_model=tabular_model_fusion, fusion_model=fusion_model_fusion)\n",
    "state_dict = torch.load(MODELS_PATH + \"/JointFusion_finetune.pth\")\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "fusion.load_state_dict(state_dict)\n",
    "\n",
    "model = HyMNet(image_model=image_model, tabular_model=tabular_model, fusion_model=fusion).cuda()\n",
    "\n",
    "state_dict = torch.load(FM_PATH)\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "state_dict = torch.load(DM_PATH)\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in state_dict.items()}\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_tabular_dataset(model, train_dataset, method=\"vf\")\n",
    "test_x, test_y = build_tabular_dataset(model, test_dataset, method=\"vf\")\n",
    "\n",
    "train_fusion_set = InputOutputDataset(train_x, train_y)\n",
    "test_fusion_set = InputOutputDataset(test_x, test_y)\n",
    "\n",
    "train_fusion_loader = DataLoader(train_fusion_set, batch_size=32)\n",
    "test_fusion_loader = DataLoader(test_fusion_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier using all scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = xgb.XGBClassifier(tree_method='gpu_hist', objective=\"binary:logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [3,6,10],\n",
    "           'learning_rate': [0.0001, 0.005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.3],\n",
    "           'n_estimators': [100, 500, 1000],\n",
    "           'colsample_bytree': [0.3, 0.7]}\n",
    "\n",
    "clf = GridSearchCV(estimator=boost, \n",
    "                   param_grid=param_grid,\n",
    "                   scoring='roc_auc', \n",
    "                   verbose=1)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODELS_PATH+'/VotingFusionXGBParams.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(clf.best_params_, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "    {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']},\n",
    "    {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001], 'degree': [1], 'kernel': ['poly']}\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(estimator=svm, \n",
    "                   param_grid=param_grid,\n",
    "                   scoring='roc_auc', \n",
    "                   verbose=3)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODELS_PATH+ '/VotingFusionSVMParams.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(clf.best_params_, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_model = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=8, out_features=32),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=32, out_features=16),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=16, out_features=8),\n",
    "    nn.LeakyReLU(inplace=True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=8, out_features=1),\n",
    ")\n",
    "\n",
    "fusion_model = fusion_model.to(device).float()\n",
    "model = HyMNet(tabular_model=fusion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': fusion_model.parameters(), 'lr': 0.005}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 250\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epoch_length * epochs, eta_min=0)\n",
    "metrics, best_model = train_val(epochs=epochs, model=model, criterion=criterion, optimizer=optimizer, train_loader=train_fusion_loader,\n",
    "                    val_loader=test_fusion_loader, scheduler=scheduler, device=device, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, MODELS_PATH + \"/VotingFusionFCNN.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retfound",
   "language": "python",
   "name": "retfound"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
