{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import getmembers, isfunction \n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, optim\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "from torchvision.models import densenet201, DenseNet201_Weights\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "import itertools\n",
    "\n",
    "import torchvision\n",
    "# import tensorflow as tf\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import splitext\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 0):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaborfilter():\n",
    "    # This function is designed to produce a set of GaborFilters \n",
    "    # an even distribution of theta values equally distributed amongst pi rad / 180 degree\n",
    "     \n",
    "    filters = []\n",
    "    num_filters = 16\n",
    "    ksize = 35  # The local area to evaluate\n",
    "    sigma = 3.0  # Larger Values produce more edges\n",
    "    lambd = 10.0\n",
    "    gamma = 0.5\n",
    "    psi = 0  # Offset value - lower generates cleaner results\n",
    "    for theta in np.arange(0, np.pi, np.pi / num_filters):  # Theta is the orientation for edge detection\n",
    "        kern = cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi, ktype=cv2.CV_64F)\n",
    "        kern /= 1.0 * kern.sum()  # Brightness normalization\n",
    "        filters.append(kern)\n",
    "    return filters\n",
    "\n",
    "def apply_filter(img, filters):\n",
    "# This general function is designed to apply filters to our image\n",
    "     \n",
    "    # First create a numpy array the same size as our input image\n",
    "    newimage = np.zeros_like(img)\n",
    "     \n",
    "    # Starting with a blank image, we loop through the images and apply our Gabor Filter\n",
    "    # On each iteration, we take the highest value (super impose), until we have the max value across all filters\n",
    "    # The final image is returned\n",
    "    depth = -1 # remain depth same as original image\n",
    "     \n",
    "    for kern in filters:  # Loop through the kernels in our GaborFilter\n",
    "        image_filter = cv2.filter2D(img, depth, kern)  #Apply filter to image\n",
    "         \n",
    "        # Using Numpy.maximum to compare our filter and cumulative image, taking the higher value (max)\n",
    "        np.maximum(newimage, image_filter, newimage)\n",
    "    return newimage\n",
    "\n",
    "def showimage(myimage, figsize=[10,10]):\n",
    "    if (myimage.ndim>2):  #This only applies to RGB or RGBA images (e.g. not to Black and White images)\n",
    "        myimage = myimage[:,:,::-1] #OpenCV follows BGR order, while matplotlib likely follows RGB order\n",
    "         \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(myimage, cmap = 'gray', interpolation = 'bicubic')\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics(metrics):\n",
    "    lenEpochs = len(metrics[0]) \n",
    "    epochs = [i for i in range(lenEpochs)]\n",
    "    train = [( metrics[0][i][\"Average Loss\"]  ) for i in metrics[0]]\n",
    "    val = [( metrics[1][i][\"Average Loss\"]  ) for i in metrics[0]]\n",
    "    plt.plot(epochs, train, label = \"Train\")\n",
    "    plt.plot(epochs, val, label = \"Validation\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend() \n",
    "    \n",
    "def show_metrics(metrics):\n",
    "    print(f'\\\n",
    "    Average Loss:  {metrics[\"Average Loss\"]:>8f},\\\n",
    "    Accuracy: {metrics[\"Accuracy\"]:>0.2f}%,\\\n",
    "    Correct Counter: {metrics[\"Correct\"]}/{metrics[\"Size\"]},\\\n",
    "    F1 Score:{metrics[\"F1 Score\"]: 0.2f},\\\n",
    "    Precision:{metrics[\"Precision\"]: 0.2f},\\\n",
    "    Recall: {metrics[\"Recall\"]: 0.2f} \\n'\n",
    "    )\n",
    "    \n",
    "    \n",
    "def split(data, train_prcntg=0.8, random=True):\n",
    "    # Calculate Testing percentage and round\n",
    "    test_prcntg = 1 - train_prcntg\n",
    "    test_prcntg = math.ceil(100 * test_prcntg) / 100\n",
    "    if random==False:\n",
    "        tr, tst = random_split(data, [math.ceil(len(data)*train_prcntg), math.floor(len(data)*test_prcntg)], generator=torch.Generator().manual_seed(0))\n",
    "    else:\n",
    "        tr, tst = random_split(data, [math.ceil(len(data)*train_prcntg), math.floor(len(data)*test_prcntg)])\n",
    "    return tr, tst \n",
    "\n",
    "\n",
    "def train_test_loader(train_set, test_set, batch_size=8):\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def get_indices(dataset, sample_ratio=0.2, seed=0):\n",
    "    \n",
    "    SAMPLE=math.floor(len(dataset) * sample_ratio)\n",
    "    \n",
    "    data = dataset.copy(deep=True)\n",
    "    test_indices = train_indices = []\n",
    "    \n",
    "    # Seeded for reproducibility\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    while len(test_indices) < SAMPLE:\n",
    "        \n",
    "        index = np.random.randint(len(data))\n",
    "        patient = data.iloc[index][\"MRN\"]\n",
    "        \n",
    "        patient_indices = data[data[\"MRN\"] == patient].index.to_list()\n",
    "        matched = dataset[dataset[\"MRN\"] == patient].index.to_list()\n",
    "            \n",
    "        test_indices += matched\n",
    "        \n",
    "        data = data.drop(data.index[patient_indices], inplace=False).reset_index(drop=True)\n",
    "\n",
    "    train_indices = [i for i in range(len(dataset)) if i not in test_indices]\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "def binary_logits_prediction(logit):\n",
    "    sig = 1/(1 + np.exp(-logit))\n",
    "    return classify(sig)\n",
    "    \n",
    "def classify(sig):\n",
    "    return int(sig>=0.5)\n",
    "\n",
    "def addFilePath(df, files):\n",
    "    indx = 0\n",
    "    for x in df.iterrows():\n",
    "        data = x[1]\n",
    "        eye = \"R\" if data[\"Eye\"] == \"OD\" else \"L\"\n",
    "        mrn = str(data[\"MRN\"])\n",
    "        for file in files:\n",
    "            if file.startswith(mrn) and eye in file:\n",
    "                df.at[indx, \"Image Path\"] = file\n",
    "                break\n",
    "        indx += 1\n",
    "    \n",
    "def get_htn_dataframe(path, normalize=True, standardize=True):    \n",
    "    \n",
    "    # Hypertension\n",
    "    htn = pd.read_excel(path[\"HTNPath\"] + r\"\\HTN_Age_Gender.xlsx\")\n",
    "    htn[\"Hypertension\"] = 1\n",
    "\n",
    "    # Clean Hypertension\n",
    "    htn.columns = htn.columns.str.strip()\n",
    "    htn[\"Eye\"] = htn[\"Eye\"].str.strip()\n",
    "\n",
    "    # Add \"File Path\" column\n",
    "    htn[\"Image Path\"] = None\n",
    "    addFilePath(htn, os.listdir(path[\"HTNPath\"]))\n",
    "\n",
    "    # non-Hypertension\n",
    "    non_htn = pd.read_excel(path[\"NonHTNPath\"] + r\"\\\\NonHTN_Age_Gender.xlsx\")\n",
    "    non_htn[\"Hypertension\"] = 0\n",
    "\n",
    "    # Clean non-Hypertension\n",
    "    non_htn.columns = non_htn.columns.str.strip()\n",
    "    non_htn[\"Eye\"] = non_htn[\"Eye\"].str.strip()\n",
    "\n",
    "    # Add \"Image Path\" column\n",
    "    non_htn[\"Image Path\"] = None\n",
    "    addFilePath(non_htn, os.listdir(path[\"NonHTNPath\"]))\n",
    "\n",
    "    # Add the two df's\n",
    "    df = pd.concat([htn, non_htn]) \n",
    "\n",
    "    # Clean again\n",
    "    df = df.dropna()\n",
    "    df[\"Gender\"] = df[\"Gender\"].str.strip()\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # One hot encoding for Gender \n",
    "    df[\"Gender\"] = (df[\"Gender\"]==\"M\").astype(\"int64\")\n",
    "        \n",
    "    if normalize:\n",
    "        df[\"Age\"]=(df[\"Age\"]-df[\"Age\"].min())/(df[\"Age\"].max()-df[\"Age\"].min())\n",
    "    if standardize:\n",
    "        df[\"Age\"] = (df[\"Age\"] - df[\"Age\"].mean())/df[\"Age\"].std(ddof=0)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "class HypertensionDataset(Dataset):\n",
    "    def __init__(self, path, df, train_transform=None, test_transform=None):\n",
    "        \n",
    "        self.path = path\n",
    "        \n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "         \n",
    "        self.df = df\n",
    "        \n",
    "        # Get X and y\n",
    "        self.X, self.y = self.df.drop([\"Hypertension\", \"MRN\", \"Eye\", \"Exam Date\"], axis=1), self.df[\"Hypertension\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Get image\n",
    "        image_path = self.X.iloc[idx][\"Image Path\"]\n",
    "        if self.y.iloc[idx] == 1:\n",
    "            image_path = self.path[\"HTNPath\"] + \"\\\\\" + image_path\n",
    "        else:\n",
    "            image_path = self.path[\"NonHTNPath\"] + \"\\\\\" + image_path\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        if self.train_transform is not None:\n",
    "            img = self.train_transform(img)\n",
    "        elif self.test_transform is not None:\n",
    "            img = self.test_transform(img)\n",
    "        \n",
    "        sample = {\n",
    "            \"image\": img,\n",
    "            \"features\": torch.from_numpy(self.X.iloc[idx][[\"Age\", \"Gender\"]].values.astype(\"float64\")),\n",
    "            \"label\": torch.tensor(self.y[idx]),\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "            \n",
    "        self.df = df\n",
    "        \n",
    "        self.X, self.y = self.df.drop([\"Hypertension\"], axis=1), self.df[\"Hypertension\"]\n",
    "        self.X.reset_index(inplace=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = {\n",
    "            \"image\": torch.tensor([]),\n",
    "            \"features\": torch.from_numpy(self.X.iloc[idx][[\"Age\", \"Gender\"]].values.astype(\"float64\")),\n",
    "            \"label\": torch.tensor(self.y[idx]),\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0.001, multip=5):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "        self.multip = multip\n",
    "        self.trigger_stop = False\n",
    "\n",
    "    def __call__(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            self.trigger_stop = False\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.trigger_stop = True\n",
    "                \n",
    "    def stop(self, epoch):\n",
    "        if epoch % self.multip == 0 and self.trigger_stop:\n",
    "            self.trigger_stop = False\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "class ApplyTransformation(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        if transform==None:\n",
    "            self.transform = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample, target = self.dataset[idx]\n",
    "        sample = self.transform(sample)\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg16(device=\"cuda:0\", freeze=False, output=1):\n",
    "    model = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    num_features = model.classifier[6].in_features\n",
    "    features = list(model.classifier.children())[:-1]\n",
    "    features.extend([nn.Linear(num_features, output)])\n",
    "    model.classifier = nn.Sequential(*features)\n",
    "    \n",
    "    if freeze:\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    return model\n",
    "\n",
    "def get_vgg19(device=\"cuda:0\", freeze=False):\n",
    "    model = vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "    model = model.to(device)\n",
    "\n",
    "    num_features = model.classifier[6].in_features\n",
    "    features = list(model.classifier.children())[:-1]\n",
    "    features.extend([nn.Linear(num_features, 1)])\n",
    "    model.classifier = nn.Sequential(*features)\n",
    "    \n",
    "    if freeze:\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_resnet50(device=\"cuda:0\", freeze=False):\n",
    "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    model = model.to(device)\n",
    "\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 1)\n",
    "    \n",
    "    if freeze:\n",
    "        model.requires_grad_(False)\n",
    "        model.fc.requires_grad_(True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_resnet152(device=\"cuda:0\", freeze=False, with_mlp=False, outputs=1):\n",
    "    model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if with_mlp:\n",
    "            model.fc = nn.Sequential(\n",
    "            model.fc,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=1000, out_features=256),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=64),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=64, out_features=outputs),\n",
    "        )\n",
    "    else:\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 1)\n",
    "    \n",
    "    if freeze:\n",
    "        model.requires_grad_(False)\n",
    "        model.fc.requires_grad_(True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_densenet201(device=\"cuda:0\", freeze=False, with_mlp=False, outputs=1):\n",
    "    model = densenet201(weights=DenseNet201_Weights.DEFAULT)\n",
    "    \n",
    "    if with_mlp:\n",
    "        model.classifier = nn.Sequential(\n",
    "            model.classifier,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=1000, out_features=256),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=64),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=64, out_features=outputs),\n",
    "        )\n",
    "    else:\n",
    "        num_features = model.classifier.in_features\n",
    "        features = list(model.classifier.children())[:-1]\n",
    "        features.extend([nn.Linear(num_features, outputs)])\n",
    "        model.classifier = nn.Sequential(*features)\n",
    "        \n",
    "    model = model.to(device)\n",
    "\n",
    "    if freeze:\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze first layer\n",
    "#         model.features.norm0.requires_grad_(True)    \n",
    "#         model.features.conv0.requires_grad_(True)    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_mobilenetv2(device=\"cuda:0\", freeze=False, with_mlp=False):\n",
    "    model = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "    if with_mlp:\n",
    "        model.classifier = nn.Sequential(\n",
    "            *model.classifier,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=1000, out_features=256),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=64),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=64, out_features=1),\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        num_features = model.classifier[1].in_features\n",
    "        features = list(model.classifier.children())[:-1]\n",
    "        features.extend([nn.Linear(num_features, 1)])\n",
    "        model.classifier = nn.Sequential(*features)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    if freeze:\n",
    "        model.features.requires_grad_(False)\n",
    "        model.classifier.requires_grad_(True)\n",
    "\n",
    "    return model\n",
    "        \n",
    "def get_inceptionv3(device=\"cuda:0\", freeze=False, with_mlp=False):\n",
    "    model = inception_v3(weights=Inception_V3_Weights.DEFAULT)\n",
    "    \n",
    "    if with_mlp:\n",
    "        model.fc = nn.Sequential(\n",
    "            model.fc,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=1000, out_features=256),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=64),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=64, out_features=1),\n",
    "        )\n",
    "    else:\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 1)\n",
    "    \n",
    "    model = model.to(device)\n",
    "        \n",
    "    if freeze:\n",
    "        model.requires_grad_(False)\n",
    "        model.fc.requires_grad_(True)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
